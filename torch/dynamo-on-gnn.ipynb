{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Dynamo Interpreter and graph breaks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will try to understand Torch-Dynamo's VM by trying some essential cases. \n",
    "All the cases are guided by the logic of its code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import _inductor, _dynamo\n",
    "from torch._inductor import config as iconfig\n",
    "from torch._dynamo import config as dconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iconfig.debug = True\n",
    "dconfig.output_code = True\n",
    "import logging\n",
    "dconfig.log_level = logging.DEBUG\n",
    "dconfig.suppress_errors = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kernel(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((3, 4), device='cuda')\n",
    "b = torch.randn((3,4), device='cuda')\n",
    "a = torch.abs(a)\n",
    "b = torch.abs(b)\n",
    "# both a and b are now positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:46,415] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /usr/lib/python3.8/contextlib.py\n",
      "[2023-04-04 10:02:46,416] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /usr/lib/python3.8/contextlib.py\n",
      "[2023-04-04 10:02:46,416] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /usr/lib/python3.8/contextlib.py\n",
      "[2023-04-04 10:02:46,417] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /usr/lib/python3.8/contextlib.py\n",
      "[2023-04-04 10:02:46,417] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py\n",
      "[2023-04-04 10:02:46,440] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing add_kernel\n",
      "[2023-04-04 10:02:46,441] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/176408799.py:2\n",
      "[2023-04-04 10:02:46,441] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:46,442] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:46,442] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:46,446] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:46,446] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing add_kernel (RETURN_VALUE)\n",
      "[2023-04-04 10:02:46,447] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:46,447] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/176408799.py, line 2 in add_kernel>])\n",
      "[2023-04-04 10:02:46,448] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:47,686] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0\n",
      "[2023-04-04 10:02:47,739] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/wp/cwp4j23ojvzq6bjw3jiiilkes5m6eiy2hm5pk4bvryqtmuzuq2z4.py\n",
      "[2023-04-04 10:02:47,740] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0\n",
      "[2023-04-04 10:02:47,741] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:47,742] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_0 <eval_with_key>.3 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/176408799.py:2, code: return a + b\n",
      "        add = a + b;  a = b = None\n",
      "        return (add,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:47,746] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_0 <eval_with_key>.3 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  add     <built-in function add>  (a, b)     {}\n",
      "output         output  output                   ((add,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:47,747] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE add_kernel /tmp/ipykernel_468424/176408799.py line 1 \n",
      "  2           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_FAST                1 (b)\n",
      "              4 BINARY_ADD\n",
      "              6 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:47,747] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE add_kernel /tmp/ipykernel_468424/176408799.py line 1 \n",
      "  1           0 LOAD_GLOBAL              0 (__compiled_fn_0)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:47,748] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:47,749] torch._dynamo.eval_frame: [DEBUG] skipping _fn /home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py\n",
      "[2023-04-04 10:02:47,749] torch._dynamo.eval_frame: [DEBUG] skipping nothing /home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py\n",
      "[2023-04-04 10:02:47,750] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /usr/lib/python3.8/contextlib.py\n",
      "[2023-04-04 10:02:47,750] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /usr/lib/python3.8/contextlib.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.8210, 1.2094, 0.9151, 1.7770],\n",
       "        [2.7160, 0.5586, 1.1959, 0.3365],\n",
       "        [2.3278, 1.1822, 0.6505, 1.3714]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = torch.compile(add_kernel)\n",
    "fn(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified code is \n",
    "\n",
    "```\n",
    "1           0 LOAD_GLOBAL              0 (__compiled_fn_0)\n",
    "              2 LOAD_FAST                0 (a)\n",
    "              4 LOAD_FAST                1 (b)\n",
    "              6 CALL_FUNCTION            2\n",
    "              8 UNPACK_SEQUENCE          1\n",
    "             10 RETURN_VALUE\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a graph break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kernel1(a, b):\n",
    "    if a.sum() > 0:\n",
    "        return a + b\n",
    "    return a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:47,874] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing add_kernel1\n",
      "[2023-04-04 10:02:47,874] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/510685252.py:2\n",
      "[2023-04-04 10:02:47,875] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:47,875] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]\n",
      "[2023-04-04 10:02:47,876] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]\n",
      "[2023-04-04 10:02:47,878] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]\n",
      "[2023-04-04 10:02:47,878] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP > [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:47,880] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 20 [TensorVariable()]\n",
      "[2023-04-04 10:02:47,880] torch._dynamo.symbolic_convert: [DEBUG] generic_jump triggered compile\n",
      "[2023-04-04 10:02:47,880] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file /tmp/ipykernel_468424/510685252.py, line 2 in add_kernel1>])\n",
      "[2023-04-04 10:02:47,881] torch._dynamo.output_graph: [DEBUG] REMOVE UNUSED GRAPHARG b\n",
      "[2023-04-04 10:02:47,882] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:47,889] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1\n",
      "[2023-04-04 10:02:47,896] torch._inductor.scheduler: [DEBUG] remove_buffer('buf0')\n",
      "[2023-04-04 10:02:47,918] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/6n/c6n4gcygqbe6jhr5lxiggwurqjiyaewzuu3l7wcidyuf4wl6r77v.py\n",
      "[2023-04-04 10:02:47,919] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1\n",
      "[2023-04-04 10:02:47,920] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:47,920] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_1 <eval_with_key>.10 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/510685252.py:2, code: if a.sum() > 0:\n",
      "        sum_1 = a.sum();  a = None\n",
      "        gt = sum_1 > 0;  sum_1 = None\n",
      "        return (gt,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:47,921] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_1 <eval_with_key>.10 opcode         name    target                  args        kwargs\n",
      "-------------  ------  ----------------------  ----------  --------\n",
      "placeholder    a       a                       ()          {}\n",
      "call_method    sum_1   sum                     (a,)        {}\n",
      "call_function  gt      <built-in function gt>  (sum_1, 0)  {}\n",
      "output         output  output                  ((gt,),)    {}\n",
      "\n",
      "[2023-04-04 10:02:47,922] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE add_kernel1 /tmp/ipykernel_468424/510685252.py line 1 \n",
      "  2           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_METHOD              0 (sum)\n",
      "              4 CALL_METHOD              0\n",
      "              6 LOAD_CONST               1 (0)\n",
      "              8 COMPARE_OP               4 (>)\n",
      "             10 POP_JUMP_IF_FALSE       20\n",
      "\n",
      "  3          12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 BINARY_ADD\n",
      "             18 RETURN_VALUE\n",
      "\n",
      "  4     >>   20 LOAD_FAST                0 (a)\n",
      "             22 LOAD_FAST                1 (b)\n",
      "             24 BINARY_SUBTRACT\n",
      "             26 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:47,923] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE add_kernel1 /tmp/ipykernel_468424/510685252.py line 1 \n",
      "  1           0 LOAD_GLOBAL              1 (__compiled_fn_1)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 CALL_FUNCTION            1\n",
      "              6 UNPACK_SEQUENCE          1\n",
      "              8 POP_JUMP_IF_FALSE       20\n",
      "             10 LOAD_GLOBAL              2 (__resume_at_12_2)\n",
      "             12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 CALL_FUNCTION            2\n",
      "             18 RETURN_VALUE\n",
      "        >>   20 LOAD_GLOBAL              3 (__resume_at_20_3)\n",
      "             22 LOAD_FAST                0 (a)\n",
      "             24 LOAD_FAST                1 (b)\n",
      "             26 CALL_FUNCTION            2\n",
      "             28 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:47,923] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:47,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in add_kernel1>\n",
      "[2023-04-04 10:02:47,926] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 14 []\n",
      "[2023-04-04 10:02:47,926] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/510685252.py:3\n",
      "[2023-04-04 10:02:47,926] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:47,927] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:47,927] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:47,929] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:47,930] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in add_kernel1> (RETURN_VALUE)\n",
      "[2023-04-04 10:02:47,930] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:47,931] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/510685252.py, line 3 in <resume in add_kernel1>>])\n",
      "[2023-04-04 10:02:47,932] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:47,940] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2\n",
      "[2023-04-04 10:02:47,945] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/wp/cwp4j23ojvzq6bjw3jiiilkes5m6eiy2hm5pk4bvryqtmuzuq2z4.py\n",
      "[2023-04-04 10:02:47,946] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2\n",
      "[2023-04-04 10:02:47,947] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:47,947] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_4 <eval_with_key>.17 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/510685252.py:3, code: return a + b\n",
      "        add = a + b;  a = b = None\n",
      "        return (add,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:47,948] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_4 <eval_with_key>.17 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  add     <built-in function add>  (a, b)     {}\n",
      "output         output  output                   ((add,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:47,949] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE <resume in add_kernel1> /tmp/ipykernel_468424/510685252.py line 2 \n",
      "  2           0 JUMP_ABSOLUTE           14\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_ATTR                0 (sum)\n",
      "              6 CALL_FUNCTION            0\n",
      "              8 LOAD_CONST               1 (0)\n",
      "             10 COMPARE_OP               4 (>)\n",
      "             12 POP_JUMP_IF_FALSE       22\n",
      "\n",
      "  3     >>   14 LOAD_FAST                0 (a)\n",
      "             16 LOAD_FAST                1 (b)\n",
      "             18 BINARY_ADD\n",
      "             20 RETURN_VALUE\n",
      "\n",
      "  4     >>   22 LOAD_FAST                0 (a)\n",
      "             24 LOAD_FAST                1 (b)\n",
      "             26 BINARY_SUBTRACT\n",
      "             28 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:47,949] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE <resume in add_kernel1> /tmp/ipykernel_468424/510685252.py line 2 \n",
      "  2           0 LOAD_GLOBAL              1 (__compiled_fn_4)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:47,950] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.8210, 1.2094, 0.9151, 1.7770],\n",
       "        [2.7160, 0.5586, 1.1959, 0.3365],\n",
       "        [2.3278, 1.1822, 0.6505, 1.3714]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dynamo.reset()\n",
    "fn = torch.compile(add_kernel1)\n",
    "fn(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:48,003] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in add_kernel1>\n",
      "[2023-04-04 10:02:48,003] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 22 []\n",
      "[2023-04-04 10:02:48,004] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/510685252.py:4\n",
      "[2023-04-04 10:02:48,004] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:48,004] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:48,005] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBTRACT None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,007] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:48,007] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in add_kernel1> (RETURN_VALUE)\n",
      "[2023-04-04 10:02:48,008] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:48,008] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/510685252.py, line 4 in <resume in add_kernel1>>])\n",
      "[2023-04-04 10:02:48,009] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:48,017] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3\n",
      "[2023-04-04 10:02:48,042] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/jh/cjhsnrywhg5wzmv4uqfxt544p35eeaaesx4veww7uzyxb5umcdeq.py\n",
      "[2023-04-04 10:02:48,042] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3\n",
      "[2023-04-04 10:02:48,043] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:48,043] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_5 <eval_with_key>.24 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/510685252.py:4, code: return a - b\n",
      "        sub = a - b;  a = b = None\n",
      "        return (sub,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:48,044] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_5 <eval_with_key>.24 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  sub     <built-in function sub>  (a, b)     {}\n",
      "output         output  output                   ((sub,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:48,044] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE <resume in add_kernel1> /tmp/ipykernel_468424/510685252.py line 2 \n",
      "  2           0 JUMP_ABSOLUTE           22\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_ATTR                0 (sum)\n",
      "              6 CALL_FUNCTION            0\n",
      "              8 LOAD_CONST               1 (0)\n",
      "             10 COMPARE_OP               4 (>)\n",
      "             12 POP_JUMP_IF_FALSE       22\n",
      "             14 LOAD_FAST                0 (a)\n",
      "             16 LOAD_FAST                1 (b)\n",
      "             18 BINARY_ADD\n",
      "             20 RETURN_VALUE\n",
      "\n",
      "  4     >>   22 LOAD_FAST                0 (a)\n",
      "             24 LOAD_FAST                1 (b)\n",
      "             26 BINARY_SUBTRACT\n",
      "             28 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,045] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE <resume in add_kernel1> /tmp/ipykernel_468424/510685252.py line 2 \n",
      "  2           0 LOAD_GLOBAL              1 (__compiled_fn_5)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,045] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8210, -1.2094, -0.9151, -1.7770],\n",
       "        [-2.7160, -0.5586, -1.1959, -0.3365],\n",
       "        [-2.3278, -1.1822, -0.6505, -1.3714]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn(-a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function above, there are two branches in the `if a.sum() > 0` control flow. \n",
    "We call it with inputs of `(a, b)` and `(-a, b)` which will trigger the then branch and the else branch repectively,\n",
    "the Inductor compilation is actived in both cases.\n",
    "\n",
    "By reading the Interpreter code, it is clear that, once a `generic_jump` hit a graph break, it will \n",
    "\n",
    "1. stop gathering the current instruction into the recent subgraph and\n",
    "    - Frozen and compile the last subgraph immediately, and put a CALL to `__compiled_xxx`\n",
    "2. create two `resume_at` function calls for both the then branch and the else branch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging into the log, we could found the original code for the `add_kernel1` function is\n",
    "\n",
    "```python\n",
    "  2           0 LOAD_FAST                0 (a)\n",
    "              2 LOAD_METHOD              0 (sum)\n",
    "              4 CALL_METHOD              0\n",
    "              6 LOAD_CONST               1 (0)\n",
    "              8 COMPARE_OP               4 (>)\n",
    "             10 POP_JUMP_IF_FALSE       20\n",
    "\n",
    "  3          12 LOAD_FAST                0 (a)\n",
    "             14 LOAD_FAST                1 (b)\n",
    "             16 BINARY_ADD\n",
    "             18 RETURN_VALUE\n",
    "\n",
    "  4     >>   20 LOAD_FAST                0 (a)\n",
    "             22 LOAD_FAST                1 (b)\n",
    "             24 BINARY_SUBTRACT\n",
    "             26 RETURN_VALUE\n",
    "```\n",
    "\n",
    "And it will first modified to the code above following the logic of `generic_jump`:\n",
    "\n",
    "\n",
    "```python\n",
    "  1           0 LOAD_GLOBAL              1 (__compiled_fn_15)\n",
    "              2 LOAD_FAST                0 (a)\n",
    "              4 CALL_FUNCTION            1\n",
    "              6 UNPACK_SEQUENCE          1\n",
    "              8 POP_JUMP_IF_FALSE       20\n",
    "             10 LOAD_GLOBAL              2 (__resume_at_12_16)\n",
    "             12 LOAD_FAST                0 (a)\n",
    "             14 LOAD_FAST                1 (b)\n",
    "             16 CALL_FUNCTION            2\n",
    "             18 RETURN_VALUE\n",
    "        >>   20 LOAD_GLOBAL              3 (__resume_at_20_17)\n",
    "             22 LOAD_FAST                0 (a)\n",
    "             24 LOAD_FAST                1 (b)\n",
    "             26 CALL_FUNCTION            2\n",
    "             28 RETURN_VALUE\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is the basic pattern of a graph break in a normal `generic_jump`.\n",
    "\n",
    "The `__compiled_fn_15` is the compilation for the condition computation, and the `__resume_at_12_16` and `__resume_at_20_17` are the if-else's then-block and else-block respectively.\n",
    "\n",
    "```python\n",
    " __compiled_fn_15 <eval_with_key>.66 class GraphModule(torch.nn.Module):\n",
    "    def forward(self, a : torch.Tensor):\n",
    "        # File: /tmp/ipykernel_69518/510685252.py:2, code: if a.sum() > 0:\n",
    "        sum_1 = a.sum();  a = None\n",
    "        gt = sum_1 > 0;  sum_1 = None\n",
    "        return (gt,)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dynamo will keep extracting subgraphs in a greedy way, even on a single op, this is a bit insane.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `resume_at_xx` blockes will be further traced, although there are two `resume_at_xx` blocks, only the one actually accessed in the current context(inputs) will be traced, the other one's bytecode is just left unchanged."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an issue: **is the tracing period actually do two things?**\n",
    "\n",
    "1. Visit each instruction, append to output_graph if necessary\n",
    "2. Evaluate the instruction, and alter the stack with real value\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is YES.\n",
    "For each instruction, it will evaluate inplace, alter the stack with the real value (wrapped in a VariableTracker), and then append the instruction to the output_graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling an unsupported op, cause a graph break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:48,099] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing kernel19\n",
      "[2023-04-04 10:02:48,100] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/115080304.py:6\n",
      "[2023-04-04 10:02:48,100] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL scatter_mul []\n",
      "[2023-04-04 10:02:48,101] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST src [UserFunctionVariable()]\n",
      "[2023-04-04 10:02:48,102] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST index [UserFunctionVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,102] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 1 [UserFunctionVariable(), TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,102] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST out [UserFunctionVariable(), TensorVariable(), TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:48,103] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST ('dim', 'out') [UserFunctionVariable(), TensorVariable(), TensorVariable(), ConstantVariable(int), TensorVariable()]\n",
      "[2023-04-04 10:02:48,103] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION_KW 4 [UserFunctionVariable(), TensorVariable(), TensorVariable(), ConstantVariable(int), TensorVariable(), ConstantVariable(tuple)]\n",
      "[2023-04-04 10:02:48,104] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object scatter_mul at 0x7f760b097190, file \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch_scatter/scatter.py\", line 32>\n",
      " 35           0 LOAD_GLOBAL              0 (torch)\n",
      "              2 LOAD_ATTR                1 (ops)\n",
      "              4 LOAD_ATTR                2 (torch_scatter)\n",
      "              6 LOAD_METHOD              3 (scatter_mul)\n",
      "              8 LOAD_FAST                0 (src)\n",
      "             10 LOAD_FAST                1 (index)\n",
      "             12 LOAD_FAST                2 (dim)\n",
      "             14 LOAD_FAST                3 (out)\n",
      "             16 LOAD_FAST                4 (dim_size)\n",
      "             18 CALL_METHOD              5\n",
      "             20 RETURN_VALUE\n",
      "\n",
      "[2023-04-04 10:02:48,104] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/chunwei/newenv2/lib/python3.8/site-packages/torch_scatter/scatter.py:35\n",
      "[2023-04-04 10:02:48,105] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-04-04 10:02:48,106] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR ops [TorchVariable(<module 'torch' from '/home/chunwei/newenv2/lib/python3.8/site-packages/torch/__init__.py'>)]\n",
      "[2023-04-04 10:02:48,107] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR torch_scatter [TorchVariable(<module 'torch.ops' from '_ops.py'>)]\n",
      "[2023-04-04 10:02:48,113] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR scatter_mul [TorchVariable(<module 'torch.ops.torch_scatter' from 'torch.ops'>)]\n",
      "[2023-04-04 10:02:48,114] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST src [TorchVariable(torch_scatter.scatter_mul)]\n",
      "[2023-04-04 10:02:48,114] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST index [TorchVariable(torch_scatter.scatter_mul), TensorVariable()]\n",
      "[2023-04-04 10:02:48,115] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST dim [TorchVariable(torch_scatter.scatter_mul), TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,115] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST out [TorchVariable(torch_scatter.scatter_mul), TensorVariable(), TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:48,115] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST dim_size [TorchVariable(torch_scatter.scatter_mul), TensorVariable(), TensorVariable(), ConstantVariable(int), TensorVariable()]\n",
      "[2023-04-04 10:02:48,116] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 5 [TorchVariable(torch_scatter.scatter_mul), TensorVariable(), TensorVariable(), ConstantVariable(int), TensorVariable(), ConstantVariable(NoneType)]\n",
      "[2023-04-04 10:02:48,122] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object scatter_mul at 0x7f760b097190, file \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch_scatter/scatter.py\", line 32>\n",
      "[2023-04-04 10:02:48,122] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 1 nodes\n",
      "[2023-04-04 10:02:48,123] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT kernel19 /tmp/ipykernel_468424/115080304.py line 5 \n",
      "due to: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_ops.py\", line 642, in __call__\n",
      "    return self._op(*args, **kwargs or {})\n",
      "RuntimeError: The tensor has a non-zero number of elements, but its data is not allocated yet. Caffe2 uses a lazy allocation, so you will need to call mutable_data() or raw_mutable_data() to actually allocate memory.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1240, in run_node\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed running call_function torch_scatter.scatter_mul(*(FakeTensor(FakeTensor(..., device='meta', size=(2, 5)), cpu), FakeTensor(FakeTensor(..., device='meta', size=(2, 5), dtype=torch.int64), cpu), 1, FakeTensor(FakeTensor(..., device='meta', size=(2, 6)), cpu), None), **{}):\n",
      "The tensor has a non-zero number of elements, but its data is not allocated yet. Caffe2 uses a lazy allocation, so you will need to call mutable_data() or raw_mutable_data() to actually allocate memory.\n",
      "(scroll up for backtrace)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1207, in get_fake_value\n",
      "    raise TorchRuntimeError() from e\n",
      "torch._dynamo.exc.TorchRuntimeError: \n",
      "\n",
      "from user code:\n",
      "   File \"/tmp/ipykernel_468424/115080304.py\", line 6, in kernel19\n",
      "    out = scatter_mul(src, index, dim=1, out=out)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch_scatter/scatter.py\", line 35, in scatter_mul\n",
      "    return torch.ops.torch_scatter.scatter_mul(src, index, dim, out, dim_size)\n",
      "\n",
      "Set torch._dynamo.config.verbose=True or TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,124] torch._dynamo.convert_frame: [INFO] converting frame raised error, suppressing error\n",
      "[2023-04-04 10:02:48,126] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing scatter_mul\n",
      "[2023-04-04 10:02:48,127] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /home/chunwei/newenv2/lib/python3.8/site-packages/torch_scatter/scatter.py:35\n",
      "[2023-04-04 10:02:48,127] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-04-04 10:02:48,128] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR ops [TorchVariable(<module 'torch' from '/home/chunwei/newenv2/lib/python3.8/site-packages/torch/__init__.py'>)]\n",
      "[2023-04-04 10:02:48,128] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR torch_scatter [TorchVariable(<module 'torch.ops' from '_ops.py'>)]\n",
      "[2023-04-04 10:02:48,136] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR scatter_mul [TorchVariable(<module 'torch.ops.torch_scatter' from 'torch.ops'>)]\n",
      "[2023-04-04 10:02:48,137] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST src [TorchVariable(torch_scatter.scatter_mul)]\n",
      "[2023-04-04 10:02:48,137] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST index [TorchVariable(torch_scatter.scatter_mul), TensorVariable()]\n",
      "[2023-04-04 10:02:48,137] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST dim [TorchVariable(torch_scatter.scatter_mul), TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,138] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST out [TorchVariable(torch_scatter.scatter_mul), TensorVariable(), TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:48,138] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST dim_size [TorchVariable(torch_scatter.scatter_mul), TensorVariable(), TensorVariable(), ConstantVariable(int), TensorVariable()]\n",
      "[2023-04-04 10:02:48,138] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 5 [TorchVariable(torch_scatter.scatter_mul), TensorVariable(), TensorVariable(), ConstantVariable(int), TensorVariable(), ConstantVariable(NoneType)]\n",
      "[2023-04-04 10:02:48,144] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT scatter_mul /home/chunwei/newenv2/lib/python3.8/site-packages/torch_scatter/scatter.py line 32 \n",
      "due to: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_ops.py\", line 642, in __call__\n",
      "    return self._op(*args, **kwargs or {})\n",
      "RuntimeError: The tensor has a non-zero number of elements, but its data is not allocated yet. Caffe2 uses a lazy allocation, so you will need to call mutable_data() or raw_mutable_data() to actually allocate memory.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1240, in run_node\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed running call_function torch_scatter.scatter_mul(*(FakeTensor(FakeTensor(..., device='meta', size=(2, 5)), cpu), FakeTensor(FakeTensor(..., device='meta', size=(2, 5), dtype=torch.int64), cpu), 1, FakeTensor(FakeTensor(..., device='meta', size=(2, 6)), cpu), None), **{}):\n",
      "The tensor has a non-zero number of elements, but its data is not allocated yet. Caffe2 uses a lazy allocation, so you will need to call mutable_data() or raw_mutable_data() to actually allocate memory.\n",
      "(scroll up for backtrace)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 1207, in get_fake_value\n",
      "    raise TorchRuntimeError() from e\n",
      "torch._dynamo.exc.TorchRuntimeError: \n",
      "\n",
      "from user code:\n",
      "   File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch_scatter/scatter.py\", line 35, in scatter_mul\n",
      "    return torch.ops.torch_scatter.scatter_mul(src, index, dim, out, dim_size)\n",
      "\n",
      "Set torch._dynamo.config.verbose=True or TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,144] torch._dynamo.convert_frame: [INFO] converting frame raised error, suppressing error\n",
      "[2023-04-04 10:02:48,144] torch._dynamo.eval_frame: [DEBUG] skipping __call__ /home/chunwei/newenv2/lib/python3.8/site-packages/torch/_ops.py\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from torch_scatter import scatter_mul\n",
    "dic = OrderedDict()\n",
    "\n",
    "def kernel19(src, index, out):\n",
    "    out = scatter_mul(src, index, dim=1, out=out)\n",
    "\n",
    "fn = torch.compile(kernel19)\n",
    "\n",
    "src = torch.Tensor([[2, 0, 1, 4, 3], [0, 2, 1, 3, 4]])\n",
    "index = torch.tensor([[4, 5, 4, 2, 3], [0, 0, 2, 2, 1]])\n",
    "out = src.new_zeros((2, 6))\n",
    "fn(src, index, out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:48,200] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing kernel20\n",
      "[2023-04-04 10:02:48,201] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/2455507784.py:3\n",
      "[2023-04-04 10:02:48,201] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:48,202] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 1.0 [TensorVariable()]\n",
      "[2023-04-04 10:02:48,202] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), ConstantVariable(float)]\n",
      "[2023-04-04 10:02:48,204] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST a [TensorVariable()]\n",
      "[2023-04-04 10:02:48,204] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/2455507784.py:4\n",
      "[2023-04-04 10:02:48,205] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:48,205] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL dic [TensorVariable()]\n",
      "[2023-04-04 10:02:48,206] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST name [TensorVariable(), ConstDictVariable()]\n",
      "[2023-04-04 10:02:48,206] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_SUBSCR None [TensorVariable(), ConstDictVariable(), ConstantVariable(str)]\n",
      "[2023-04-04 10:02:48,207] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/2455507784.py:5\n",
      "[2023-04-04 10:02:48,207] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:48,207] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 2 [TensorVariable()]\n",
      "[2023-04-04 10:02:48,208] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_MULTIPLY None [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:48,210] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST a [TensorVariable()]\n",
      "[2023-04-04 10:02:48,210] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/2455507784.py:6\n",
      "[2023-04-04 10:02:48,210] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:48,211] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:48,211] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing kernel20 (RETURN_VALUE)\n",
      "[2023-04-04 10:02:48,211] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:48,212] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/2455507784.py, line 6 in kernel20>])\n",
      "[2023-04-04 10:02:48,214] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:48,225] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4\n",
      "[2023-04-04 10:02:48,250] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/pc/cpct4jbiedytstoki3yku3njyw7qf7ccbfatgd4s37nnfgdh4t46.py\n",
      "[2023-04-04 10:02:48,251] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4\n",
      "[2023-04-04 10:02:48,252] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:48,252] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_6 <eval_with_key>.31 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/2455507784.py:3, code: a = a + 1.\n",
      "        add = a + 1.0;  a = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_468424/2455507784.py:5, code: a = a * 2\n",
      "        mul = add * 2\n",
      "        return (mul, add)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:48,253] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_6 <eval_with_key>.31 opcode         name    target                   args           kwargs\n",
      "-------------  ------  -----------------------  -------------  --------\n",
      "placeholder    a       a                        ()             {}\n",
      "call_function  add     <built-in function add>  (a, 1.0)       {}\n",
      "call_function  mul     <built-in function mul>  (add, 2)       {}\n",
      "output         output  output                   ((mul, add),)  {}\n",
      "\n",
      "[2023-04-04 10:02:48,254] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE kernel20 /tmp/ipykernel_468424/2455507784.py line 2 \n",
      "  3           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_CONST               1 (1.0)\n",
      "              4 BINARY_ADD\n",
      "              6 STORE_FAST               0 (a)\n",
      "\n",
      "  4           8 LOAD_FAST                0 (a)\n",
      "             10 LOAD_GLOBAL              0 (dic)\n",
      "             12 LOAD_CONST               2 ('name')\n",
      "             14 STORE_SUBSCR\n",
      "\n",
      "  5          16 LOAD_FAST                0 (a)\n",
      "             18 LOAD_CONST               3 (2)\n",
      "             20 BINARY_MULTIPLY\n",
      "             22 STORE_FAST               0 (a)\n",
      "\n",
      "  6          24 LOAD_FAST                0 (a)\n",
      "             26 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,254] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE kernel20 /tmp/ipykernel_468424/2455507784.py line 2 \n",
      "  2           0 LOAD_GLOBAL              5 (__compiled_fn_6)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 CALL_FUNCTION            1\n",
      "              6 STORE_FAST               1 (___graph_out_0)\n",
      "              8 LOAD_FAST                1 (___graph_out_0)\n",
      "             10 LOAD_CONST               4 (0)\n",
      "             12 BINARY_SUBSCR\n",
      "             14 LOAD_GLOBAL              0 (dic)\n",
      "             16 LOAD_METHOD              2 (update)\n",
      "             18 LOAD_GLOBAL              3 (___module_collections_140148317424512)\n",
      "             20 LOAD_ATTR                4 (OrderedDict)\n",
      "             22 LOAD_CONST               2 ('name')\n",
      "             24 LOAD_FAST                1 (___graph_out_0)\n",
      "             26 LOAD_CONST               5 (1)\n",
      "             28 BINARY_SUBSCR\n",
      "             30 BUILD_MAP                1\n",
      "             32 CALL_FUNCTION            1\n",
      "             34 LOAD_GLOBAL              0 (dic)\n",
      "             36 LOAD_METHOD              1 (clear)\n",
      "             38 CALL_METHOD              0\n",
      "             40 POP_TOP\n",
      "             42 CALL_METHOD              1\n",
      "             44 POP_TOP\n",
      "             46 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,335] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'dic' DICT_KEYS\n",
      "            {\n",
      "                'guard_types': ['DICT_KEYS'],\n",
      "                'code': ['___check_type_id(dic, 9472704)', 'set(dic.keys()) == set()'],\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': <weakref at 0x7f76d2c7adb0; to 'type' at 0x908ac0 (OrderedDict)>\n",
      "            }\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.1813, 2.9671, 2.2853, 3.1537],\n",
       "        [4.0779, 2.9101, 2.6452, 2.5583],\n",
       "        [3.2492, 3.0482, 3.0148, 2.7530]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = OrderedDict()\n",
    "def kernel20(a):\n",
    "    a = a + 1.\n",
    "    dic[\"name\"] = a\n",
    "    a = a * 2\n",
    "    return a\n",
    "\n",
    "fn = torch.compile(kernel20)\n",
    "fn(a)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How could FX graph capture the ops?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `call_function` will be captured by the BuiltinVariable, and once the calling target matches `is_allowed`(check that it is inside Torch), the `output` will add a node in its subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kernel2(a, b):\n",
    "    a0 = 0\n",
    "    a1 = 1\n",
    "    a2 = 2\n",
    "    a3 = a0 + a1 + a2\n",
    "    if a.sum() > 0:\n",
    "        return a + b + a3\n",
    "    return a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dynamo.reset()\n",
    "fn = torch.compile(add_kernel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:06:04,970] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing add_kernel2\n",
      "[2023-04-04 10:06:04,970] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1377176595.py:2\n",
      "[2023-04-04 10:06:04,971] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 []\n",
      "[2023-04-04 10:06:04,971] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST a0 [ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,972] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1377176595.py:3\n",
      "[2023-04-04 10:06:04,972] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 1 []\n",
      "[2023-04-04 10:06:04,972] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST a1 [ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,973] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1377176595.py:4\n",
      "[2023-04-04 10:06:04,973] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 2 []\n",
      "[2023-04-04 10:06:04,974] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST a2 [ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,974] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1377176595.py:5\n",
      "[2023-04-04 10:06:04,975] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a0 []\n",
      "[2023-04-04 10:06:04,975] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a1 [ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,975] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [ConstantVariable(int), ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,976] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a2 [ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,976] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [ConstantVariable(int), ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,977] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST a3 [ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,977] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1377176595.py:6\n",
      "[2023-04-04 10:06:04,977] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:06:04,978] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]\n",
      "[2023-04-04 10:06:04,978] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]\n",
      "[2023-04-04 10:06:04,981] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]\n",
      "[2023-04-04 10:06:04,981] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP > [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:06:04,983] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 48 [TensorVariable()]\n",
      "[2023-04-04 10:06:04,983] torch._dynamo.symbolic_convert: [DEBUG] generic_jump triggered compile\n",
      "[2023-04-04 10:06:04,984] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file /tmp/ipykernel_468424/1377176595.py, line 6 in add_kernel2>])\n",
      "[2023-04-04 10:06:04,984] torch._dynamo.output_graph: [DEBUG] REMOVE UNUSED GRAPHARG b\n",
      "[2023-04-04 10:06:04,985] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:06:04,994] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19\n",
      "[2023-04-04 10:06:05,000] torch._inductor.scheduler: [DEBUG] remove_buffer('buf0')\n",
      "[2023-04-04 10:06:05,001] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/6n/c6n4gcygqbe6jhr5lxiggwurqjiyaewzuu3l7wcidyuf4wl6r77v.py\n",
      "[2023-04-04 10:06:05,002] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19\n",
      "[2023-04-04 10:06:05,002] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:06:05,003] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_30 <eval_with_key>.136 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/1377176595.py:6, code: if a.sum() > 0:\n",
      "        sum_1 = a.sum();  a = None\n",
      "        gt = sum_1 > 0;  sum_1 = None\n",
      "        return (gt,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:06:05,003] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_30 <eval_with_key>.136 opcode         name    target                  args        kwargs\n",
      "-------------  ------  ----------------------  ----------  --------\n",
      "placeholder    a       a                       ()          {}\n",
      "call_method    sum_1   sum                     (a,)        {}\n",
      "call_function  gt      <built-in function gt>  (sum_1, 0)  {}\n",
      "output         output  output                  ((gt,),)    {}\n",
      "\n",
      "[2023-04-04 10:06:05,005] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE add_kernel2 /tmp/ipykernel_468424/1377176595.py line 1 \n",
      "  2           0 LOAD_CONST               1 (0)\n",
      "              2 STORE_FAST               2 (a0)\n",
      "\n",
      "  3           4 LOAD_CONST               2 (1)\n",
      "              6 STORE_FAST               3 (a1)\n",
      "\n",
      "  4           8 LOAD_CONST               3 (2)\n",
      "             10 STORE_FAST               4 (a2)\n",
      "\n",
      "  5          12 LOAD_FAST                2 (a0)\n",
      "             14 LOAD_FAST                3 (a1)\n",
      "             16 BINARY_ADD\n",
      "             18 LOAD_FAST                4 (a2)\n",
      "             20 BINARY_ADD\n",
      "             22 STORE_FAST               5 (a3)\n",
      "\n",
      "  6          24 LOAD_FAST                0 (a)\n",
      "             26 LOAD_METHOD              0 (sum)\n",
      "             28 CALL_METHOD              0\n",
      "             30 LOAD_CONST               1 (0)\n",
      "             32 COMPARE_OP               4 (>)\n",
      "             34 POP_JUMP_IF_FALSE       48\n",
      "\n",
      "  7          36 LOAD_FAST                0 (a)\n",
      "             38 LOAD_FAST                1 (b)\n",
      "             40 BINARY_ADD\n",
      "             42 LOAD_FAST                5 (a3)\n",
      "             44 BINARY_ADD\n",
      "             46 RETURN_VALUE\n",
      "\n",
      "  8     >>   48 LOAD_FAST                0 (a)\n",
      "             50 LOAD_FAST                1 (b)\n",
      "             52 BINARY_SUBTRACT\n",
      "             54 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:06:05,006] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE add_kernel2 /tmp/ipykernel_468424/1377176595.py line 1 \n",
      "  1           0 LOAD_GLOBAL              1 (__compiled_fn_30)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 CALL_FUNCTION            1\n",
      "              6 STORE_FAST               6 (___graph_out_0)\n",
      "              8 LOAD_FAST                6 (___graph_out_0)\n",
      "             10 LOAD_CONST               1 (0)\n",
      "             12 BINARY_SUBSCR\n",
      "             14 LOAD_CONST               4 (3)\n",
      "             16 STORE_FAST               5 (a3)\n",
      "             18 POP_JUMP_IF_FALSE       32\n",
      "             20 LOAD_GLOBAL              2 (__resume_at_36_31)\n",
      "             22 LOAD_FAST                0 (a)\n",
      "             24 LOAD_FAST                1 (b)\n",
      "             26 LOAD_FAST                5 (a3)\n",
      "             28 CALL_FUNCTION            3\n",
      "             30 RETURN_VALUE\n",
      "        >>   32 LOAD_GLOBAL              3 (__resume_at_48_32)\n",
      "             34 LOAD_FAST                0 (a)\n",
      "             36 LOAD_FAST                1 (b)\n",
      "             38 CALL_FUNCTION            2\n",
      "             40 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:06:05,006] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:06:05,017] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in add_kernel2>\n",
      "[2023-04-04 10:06:05,018] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 38 []\n",
      "[2023-04-04 10:06:05,018] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1377176595.py:7\n",
      "[2023-04-04 10:06:05,018] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:06:05,019] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:06:05,019] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:06:05,021] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a3 [TensorVariable()]\n",
      "[2023-04-04 10:06:05,021] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:06:05,022] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:06:05,022] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in add_kernel2> (RETURN_VALUE)\n",
      "[2023-04-04 10:06:05,023] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:06:05,023] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/1377176595.py, line 7 in <resume in add_kernel2>>])\n",
      "[2023-04-04 10:06:05,024] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:06:05,032] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20\n",
      "[2023-04-04 10:06:05,038] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/6y/c6ygdnbke2s7hksfib5oga472idzewifpptb2day63ajeaso437p.py\n",
      "[2023-04-04 10:06:05,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20\n",
      "[2023-04-04 10:06:05,039] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:06:05,039] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_33 <eval_with_key>.143 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/1377176595.py:7, code: return a + b + a3\n",
      "        add = a + b;  a = b = None\n",
      "        add_1 = add + 3;  add = None\n",
      "        return (add_1,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:06:05,040] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_33 <eval_with_key>.143 opcode         name    target                   args         kwargs\n",
      "-------------  ------  -----------------------  -----------  --------\n",
      "placeholder    a       a                        ()           {}\n",
      "placeholder    b       b                        ()           {}\n",
      "call_function  add     <built-in function add>  (a, b)       {}\n",
      "call_function  add_1   <built-in function add>  (add, 3)     {}\n",
      "output         output  output                   ((add_1,),)  {}\n",
      "\n",
      "[2023-04-04 10:06:05,040] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE <resume in add_kernel2> /tmp/ipykernel_468424/1377176595.py line 6 \n",
      "  6           0 JUMP_ABSOLUTE           38\n",
      "              2 LOAD_CONST               1 (0)\n",
      "              4 STORE_FAST               3 (a0)\n",
      "              6 LOAD_CONST               2 (1)\n",
      "              8 STORE_FAST               4 (a1)\n",
      "             10 LOAD_CONST               3 (2)\n",
      "             12 STORE_FAST               5 (a2)\n",
      "             14 LOAD_FAST                3 (a0)\n",
      "             16 LOAD_FAST                4 (a1)\n",
      "             18 BINARY_ADD\n",
      "             20 LOAD_FAST                5 (a2)\n",
      "             22 BINARY_ADD\n",
      "             24 STORE_FAST               2 (a3)\n",
      "             26 LOAD_FAST                0 (a)\n",
      "             28 LOAD_ATTR                0 (sum)\n",
      "             30 CALL_FUNCTION            0\n",
      "             32 LOAD_CONST               1 (0)\n",
      "             34 COMPARE_OP               4 (>)\n",
      "             36 POP_JUMP_IF_FALSE       50\n",
      "\n",
      "  7     >>   38 LOAD_FAST                0 (a)\n",
      "             40 LOAD_FAST                1 (b)\n",
      "             42 BINARY_ADD\n",
      "             44 LOAD_FAST                2 (a3)\n",
      "             46 BINARY_ADD\n",
      "             48 RETURN_VALUE\n",
      "\n",
      "  8     >>   50 LOAD_FAST                0 (a)\n",
      "             52 LOAD_FAST                1 (b)\n",
      "             54 BINARY_SUBTRACT\n",
      "             56 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:06:05,041] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE <resume in add_kernel2> /tmp/ipykernel_468424/1377176595.py line 6 \n",
      "  6           0 LOAD_GLOBAL              1 (__compiled_fn_33)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:06:05,042] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'a3' CONSTANT_MATCH\n",
      "            {\n",
      "                'guard_types': ['EQUALS_MATCH'],\n",
      "                'code': ['___check_type_id(a3, 9487360)', 'a3 == 3'],\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': <weakref at 0x7f76d2c6a090; to 'type' at 0x90c400 (int)>\n",
      "            }\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[14.4727,  7.8387,  7.7772, 10.7776],\n",
       "        [14.1011,  4.0767,  8.5622,  3.6233],\n",
       "        [13.8441,  7.4727,  4.3660,  9.3458]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn(a,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first subgraph, the FX graph is\n",
    "\n",
    "```python\n",
    " __compiled_fn_15 <eval_with_key>.66 class GraphModule(torch.nn.Module):\n",
    "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
    "        # File: /tmp/ipykernel_242802/1377176595.py:7, code: return a + b + a3\n",
    "        add = a + b;  a = b = None\n",
    "        add_1 = add + 3;  add = None\n",
    "        return (add_1,)\n",
    "```\n",
    "The **3** comes from the tracing of the logic\n",
    "\n",
    "```python\n",
    "    a0 = 0\n",
    "    a1 = 1\n",
    "    a2 = 2\n",
    "    a3 = a0 + a1 + a2\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tracer keeps evaluate the non-torch-op instructions, and update the frame stack, so that the non-tensor computations will be replaced with constant value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip frame if graph break in a loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the interpreter's code, Dynamo will skip tracing a frame once it hit a graph break within a forloop/while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kernel3(a, b):\n",
    "    for i in range(5):\n",
    "        sum = a\n",
    "        if a.sum() > 0:\n",
    "            sum += b\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = torch.compile(add_kernel3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:48,762] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing add_kernel3\n",
      "[2023-04-04 10:02:48,763] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1226052577.py:2\n",
      "[2023-04-04 10:02:48,764] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL range []\n",
      "[2023-04-04 10:02:48,765] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 5 [BuiltinVariable(range)]\n",
      "[2023-04-04 10:02:48,765] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [BuiltinVariable(range), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:48,766] torch._dynamo.symbolic_convert: [DEBUG] TRACE GET_ITER None [RangeVariable()]\n",
      "[2023-04-04 10:02:48,767] torch._dynamo.symbolic_convert: [DEBUG] TRACE FOR_ITER 38 [ListIteratorVariable()]\n",
      "[2023-04-04 10:02:48,768] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST i [ListIteratorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:48,768] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1226052577.py:3\n",
      "[2023-04-04 10:02:48,769] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [ListIteratorVariable()]\n",
      "[2023-04-04 10:02:48,769] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST sum [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,769] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1226052577.py:4\n",
      "[2023-04-04 10:02:48,770] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [ListIteratorVariable()]\n",
      "[2023-04-04 10:02:48,770] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,771] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [ListIteratorVariable(), GetAttrVariable(TensorVariable(), sum)]\n",
      "[2023-04-04 10:02:48,774] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,774] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP > [ListIteratorVariable(), TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:48,776] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 8 [ListIteratorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,776] torch._dynamo.symbolic_convert: [INFO] Skipping frame because there is a graph break in a for/while loop\n",
      "[2023-04-04 10:02:48,777] torch._dynamo.convert_frame: [DEBUG] Skipping frame Skipping frame because there is a graph break in a for/while loop add_kernel3                     /tmp/ipykernel_468424/1226052577.py 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9.7424, 4.1129, 4.0048, 6.5775],\n",
       "        [9.4241, 0.9731, 4.6890, 0.5659],\n",
       "        [9.1408, 3.8146, 1.2229, 5.3509]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dynamo.reset()\n",
    "fn(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got `[2023-03-30 13:23:10,277] torch._dynamo.symbolic_convert: [INFO] Skipping frame because there is a graph break in a for/while loop` in the log, the whole frame is skipped by the tracer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Guard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:48,828] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing simple_kernel\n",
      "[2023-04-04 10:02:48,829] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/4251916827.py:2\n",
      "[2023-04-04 10:02:48,829] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST actived []\n",
      "[2023-04-04 10:02:48,830] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 12 [ConstantVariable(bool)]\n",
      "[2023-04-04 10:02:48,830] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/4251916827.py:3\n",
      "[2023-04-04 10:02:48,831] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:48,831] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:48,832] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,834] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:48,835] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing simple_kernel (RETURN_VALUE)\n",
      "[2023-04-04 10:02:48,835] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:48,836] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/4251916827.py, line 3 in simple_kernel>])\n",
      "[2023-04-04 10:02:48,837] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:48,846] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7\n",
      "[2023-04-04 10:02:48,853] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/wp/cwp4j23ojvzq6bjw3jiiilkes5m6eiy2hm5pk4bvryqtmuzuq2z4.py\n",
      "[2023-04-04 10:02:48,853] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7\n",
      "[2023-04-04 10:02:48,854] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:48,855] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_11 <eval_with_key>.52 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/4251916827.py:3, code: return a + b\n",
      "        add = a + b;  a = b = None\n",
      "        return (add,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:48,856] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_11 <eval_with_key>.52 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  add     <built-in function add>  (a, b)     {}\n",
      "output         output  output                   ((add,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:48,856] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE simple_kernel /tmp/ipykernel_468424/4251916827.py line 1 \n",
      "  2           0 LOAD_FAST                2 (actived)\n",
      "              2 POP_JUMP_IF_FALSE       12\n",
      "\n",
      "  3           4 LOAD_FAST                0 (a)\n",
      "              6 LOAD_FAST                1 (b)\n",
      "              8 BINARY_ADD\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "  5     >>   12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 BINARY_SUBTRACT\n",
      "             18 RETURN_VALUE\n",
      "             20 LOAD_CONST               0 (None)\n",
      "             22 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,857] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE simple_kernel /tmp/ipykernel_468424/4251916827.py line 1 \n",
      "  1           0 LOAD_GLOBAL              0 (__compiled_fn_11)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,859] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'actived' CONSTANT_MATCH\n",
      "            {\n",
      "                'guard_types': ['ID_MATCH'],\n",
      "                'code': ['___check_obj_id(actived, 9480544)'],\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': <weakref at 0x7f76d2c6a360; to 'type' at 0x90a980 (bool)>\n",
      "            }\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[11.4727,  4.8387,  4.7772,  7.7776],\n",
       "        [11.1011,  1.0767,  5.5622,  0.6233],\n",
       "        [10.8441,  4.4727,  1.3660,  6.3458]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_kernel(a, b, actived:bool):\n",
    "    if actived:\n",
    "        return a + b\n",
    "    else:\n",
    "        return a - b\n",
    "\n",
    "fn = torch.compile(simple_kernel)\n",
    "fn(a, b, True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will Dynamo always inline call functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:48,903] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func1\n",
      "[2023-04-04 10:02:48,904] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1059658919.py:5\n",
      "[2023-04-04 10:02:48,904] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:48,905] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:48,905] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBTRACT None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,907] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST sum [TensorVariable()]\n",
      "[2023-04-04 10:02:48,908] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1059658919.py:6\n",
      "[2023-04-04 10:02:48,908] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST sum []\n",
      "[2023-04-04 10:02:48,909] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL func0 [TensorVariable()]\n",
      "[2023-04-04 10:02:48,909] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [TensorVariable(), UserFunctionVariable()]\n",
      "[2023-04-04 10:02:48,910] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable(), UserFunctionVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,910] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [TensorVariable(), UserFunctionVariable(), TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,911] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object func0 at 0x7f7606efd870, file \"/tmp/ipykernel_468424/1059658919.py\", line 1>\n",
      "  2           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_FAST                1 (b)\n",
      "              4 BINARY_ADD\n",
      "              6 RETURN_VALUE\n",
      "\n",
      "[2023-04-04 10:02:48,911] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1059658919.py:2\n",
      "[2023-04-04 10:02:48,911] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:48,912] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:48,912] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,914] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:48,915] torch._dynamo.symbolic_convert: [DEBUG] DONE INLINING <code object func0 at 0x7f7606efd870, file \"/tmp/ipykernel_468424/1059658919.py\", line 1>\n",
      "[2023-04-04 10:02:48,915] torch._dynamo.symbolic_convert: [DEBUG] TRACE INPLACE_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:48,916] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST sum [TensorVariable()]\n",
      "[2023-04-04 10:02:48,917] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/1059658919.py:7\n",
      "[2023-04-04 10:02:48,917] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST sum []\n",
      "[2023-04-04 10:02:48,917] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:48,918] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing func1 (RETURN_VALUE)\n",
      "[2023-04-04 10:02:48,918] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:48,918] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/1059658919.py, line 7 in func1>])\n",
      "[2023-04-04 10:02:48,920] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:48,934] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8\n",
      "[2023-04-04 10:02:48,961] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/jv/cjvyqd3sbbkwrwv2xoms3kciw62hn3bfymffgpenkpnughkdpf7c.py\n",
      "[2023-04-04 10:02:48,962] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8\n",
      "[2023-04-04 10:02:48,963] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:48,963] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_12 <eval_with_key>.59 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/1059658919.py:5, code: sum = a - b\n",
      "        sub = a - b\n",
      "        \n",
      "        # File: /tmp/ipykernel_468424/1059658919.py:2, code: return a + b\n",
      "        add = a + b;  a = b = None\n",
      "        \n",
      "        # File: /tmp/ipykernel_468424/1059658919.py:6, code: sum += func0(a, b)\n",
      "        sub += add;  iadd = sub;  sub = add = None\n",
      "        return (iadd,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:48,964] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_12 <eval_with_key>.59 opcode         name    target                    args        kwargs\n",
      "-------------  ------  ------------------------  ----------  --------\n",
      "placeholder    a       a                         ()          {}\n",
      "placeholder    b       b                         ()          {}\n",
      "call_function  sub     <built-in function sub>   (a, b)      {}\n",
      "call_function  add     <built-in function add>   (a, b)      {}\n",
      "call_function  iadd    <built-in function iadd>  (sub, add)  {}\n",
      "output         output  output                    ((iadd,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:48,965] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE func1 /tmp/ipykernel_468424/1059658919.py line 4 \n",
      "  5           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_FAST                1 (b)\n",
      "              4 BINARY_SUBTRACT\n",
      "              6 STORE_FAST               2 (sum)\n",
      "\n",
      "  6           8 LOAD_FAST                2 (sum)\n",
      "             10 LOAD_GLOBAL              0 (func0)\n",
      "             12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 CALL_FUNCTION            2\n",
      "             18 INPLACE_ADD\n",
      "             20 STORE_FAST               2 (sum)\n",
      "\n",
      "  7          22 LOAD_FAST                2 (sum)\n",
      "             24 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,966] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE func1 /tmp/ipykernel_468424/1059658919.py line 4 \n",
      "  4           0 LOAD_GLOBAL              1 (__compiled_fn_12)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:48,966] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'func0' FUNCTION_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[19.4847,  8.2257,  8.0096, 13.1550],\n",
       "        [18.8481,  1.9462,  9.3779,  1.1318],\n",
       "        [18.2816,  7.6292,  2.4458, 10.7018]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func0(a, b):\n",
    "    return a + b\n",
    "\n",
    "def func1(a, b):\n",
    "    sum = a - b\n",
    "    sum += func0(a, b)\n",
    "    return sum\n",
    "\n",
    "fn = torch.compile(func1)\n",
    "\n",
    "fn(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could find \n",
    "\n",
    "1. `[DEBUG] INLINING <code object func0 at 0x7f147d36e660, file \"/tmp/ipykernel_354122/1059658919.py\", line 1>` in the log, which means that the tracer trys to inline the func0\n",
    "2. `DONE INLINING <code object func0 at 0x7f147d36e660, file \"/tmp/ipykernel_354122/1059658919.py\", line 1>` which means func0 is successfully inlined into func1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any cases the INLINE will fail?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: the helper function has a graph break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:49,013] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func1\n",
      "[2023-04-04 10:02:49,013] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/3110369299.py:7\n",
      "[2023-04-04 10:02:49,014] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,014] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:49,015] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBTRACT None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,017] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST res [TensorVariable()]\n",
      "[2023-04-04 10:02:49,017] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/3110369299.py:8\n",
      "[2023-04-04 10:02:49,017] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST res []\n",
      "[2023-04-04 10:02:49,018] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL func0 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,018] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [TensorVariable(), UserFunctionVariable()]\n",
      "[2023-04-04 10:02:49,019] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable(), UserFunctionVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,019] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [TensorVariable(), UserFunctionVariable(), TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,020] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object func0 at 0x7f7606e2b5b0, file \"/tmp/ipykernel_468424/3110369299.py\", line 1>\n",
      "  2           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_METHOD              0 (sum)\n",
      "              4 CALL_METHOD              0\n",
      "              6 LOAD_CONST               1 (0)\n",
      "              8 COMPARE_OP               4 (>)\n",
      "             10 POP_JUMP_IF_FALSE       20\n",
      "\n",
      "  3          12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 BINARY_ADD\n",
      "             18 RETURN_VALUE\n",
      "\n",
      "  4     >>   20 LOAD_FAST                0 (a)\n",
      "             22 LOAD_FAST                1 (b)\n",
      "             24 BINARY_SUBTRACT\n",
      "             26 RETURN_VALUE\n",
      "\n",
      "[2023-04-04 10:02:49,020] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/3110369299.py:2\n",
      "[2023-04-04 10:02:49,020] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,021] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]\n",
      "[2023-04-04 10:02:49,021] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]\n",
      "[2023-04-04 10:02:49,023] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,024] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP > [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:49,025] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 20 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,026] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object func0 at 0x7f7606e2b5b0, file \"/tmp/ipykernel_468424/3110369299.py\", line 1>\n",
      "[2023-04-04 10:02:49,026] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 2 nodes\n",
      "[2023-04-04 10:02:49,027] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 347, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1003, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 495, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/variables/functions.py\", line 265, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/variables/functions.py\", line 98, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 531, in inline_user_function_return\n",
      "    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1947, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 2011, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 604, in run\n",
      "    and self.step()\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 564, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 332, in inner\n",
      "    unimplemented(f\"generic_jump {typestr(value)}\")\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/exc.py\", line 78, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: generic_jump TensorVariable()\n",
      "[2023-04-04 10:02:49,028] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes\n",
      "[2023-04-04 10:02:49,029] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file /tmp/ipykernel_468424/3110369299.py, line 8 in func1>, <FrameSummary file /tmp/ipykernel_468424/3110369299.py, line 2 in func0>])\n",
      "[2023-04-04 10:02:49,030] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,038] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9\n",
      "[2023-04-04 10:02:49,045] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/jh/cjhsnrywhg5wzmv4uqfxt544p35eeaaesx4veww7uzyxb5umcdeq.py\n",
      "[2023-04-04 10:02:49,045] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9\n",
      "[2023-04-04 10:02:49,046] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,047] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_13 <eval_with_key>.66 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/3110369299.py:7, code: res = a - b\n",
      "        sub = a - b;  a = b = None\n",
      "        return (sub,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,048] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_13 <eval_with_key>.66 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  sub     <built-in function sub>  (a, b)     {}\n",
      "output         output  output                   ((sub,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:49,049] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE func1 /tmp/ipykernel_468424/3110369299.py line 6 \n",
      "  7           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_FAST                1 (b)\n",
      "              4 BINARY_SUBTRACT\n",
      "              6 STORE_FAST               2 (res)\n",
      "\n",
      "  8           8 LOAD_FAST                2 (res)\n",
      "             10 LOAD_GLOBAL              0 (func0)\n",
      "             12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 CALL_FUNCTION            2\n",
      "             18 INPLACE_ADD\n",
      "             20 STORE_FAST               2 (res)\n",
      "\n",
      "  9          22 LOAD_FAST                2 (res)\n",
      "             24 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,049] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE func1 /tmp/ipykernel_468424/3110369299.py line 6 \n",
      "  6           0 LOAD_GLOBAL              1 (__compiled_fn_13)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 STORE_FAST               3 (___graph_out_0)\n",
      "             10 LOAD_FAST                3 (___graph_out_0)\n",
      "             12 LOAD_CONST               1 (0)\n",
      "             14 BINARY_SUBSCR\n",
      "             16 LOAD_GLOBAL              0 (func0)\n",
      "             18 LOAD_FAST                0 (a)\n",
      "             20 LOAD_FAST                1 (b)\n",
      "\n",
      "  8          22 CALL_FUNCTION            2\n",
      "             24 LOAD_GLOBAL              2 (__resume_at_18_14)\n",
      "             26 ROT_THREE\n",
      "             28 CALL_FUNCTION            2\n",
      "             30 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,050] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'func0' FUNCTION_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func0\n",
      "[2023-04-04 10:02:49,053] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/3110369299.py:2\n",
      "[2023-04-04 10:02:49,053] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,054] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]\n",
      "[2023-04-04 10:02:49,054] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]\n",
      "[2023-04-04 10:02:49,055] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,056] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP > [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:49,057] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 20 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,057] torch._dynamo.symbolic_convert: [DEBUG] generic_jump triggered compile\n",
      "[2023-04-04 10:02:49,058] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file /tmp/ipykernel_468424/3110369299.py, line 2 in func0>])\n",
      "[2023-04-04 10:02:49,058] torch._dynamo.output_graph: [DEBUG] REMOVE UNUSED GRAPHARG b\n",
      "[2023-04-04 10:02:49,060] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,068] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10\n",
      "[2023-04-04 10:02:49,074] torch._inductor.scheduler: [DEBUG] remove_buffer('buf0')\n",
      "[2023-04-04 10:02:49,075] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/6n/c6n4gcygqbe6jhr5lxiggwurqjiyaewzuu3l7wcidyuf4wl6r77v.py\n",
      "[2023-04-04 10:02:49,076] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10\n",
      "[2023-04-04 10:02:49,076] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,077] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_15 <eval_with_key>.73 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/3110369299.py:2, code: if a.sum() > 0:\n",
      "        sum_1 = a.sum();  a = None\n",
      "        gt = sum_1 > 0;  sum_1 = None\n",
      "        return (gt,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,078] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_15 <eval_with_key>.73 opcode         name    target                  args        kwargs\n",
      "-------------  ------  ----------------------  ----------  --------\n",
      "placeholder    a       a                       ()          {}\n",
      "call_method    sum_1   sum                     (a,)        {}\n",
      "call_function  gt      <built-in function gt>  (sum_1, 0)  {}\n",
      "output         output  output                  ((gt,),)    {}\n",
      "\n",
      "[2023-04-04 10:02:49,079] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE func0 /tmp/ipykernel_468424/3110369299.py line 1 \n",
      "  2           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_METHOD              0 (sum)\n",
      "              4 CALL_METHOD              0\n",
      "              6 LOAD_CONST               1 (0)\n",
      "              8 COMPARE_OP               4 (>)\n",
      "             10 POP_JUMP_IF_FALSE       20\n",
      "\n",
      "  3          12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 BINARY_ADD\n",
      "             18 RETURN_VALUE\n",
      "\n",
      "  4     >>   20 LOAD_FAST                0 (a)\n",
      "             22 LOAD_FAST                1 (b)\n",
      "             24 BINARY_SUBTRACT\n",
      "             26 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,079] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE func0 /tmp/ipykernel_468424/3110369299.py line 1 \n",
      "  1           0 LOAD_GLOBAL              1 (__compiled_fn_15)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 CALL_FUNCTION            1\n",
      "              6 UNPACK_SEQUENCE          1\n",
      "              8 POP_JUMP_IF_FALSE       20\n",
      "             10 LOAD_GLOBAL              2 (__resume_at_12_16)\n",
      "             12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 CALL_FUNCTION            2\n",
      "             18 RETURN_VALUE\n",
      "        >>   20 LOAD_GLOBAL              3 (__resume_at_20_17)\n",
      "             22 LOAD_FAST                0 (a)\n",
      "             24 LOAD_FAST                1 (b)\n",
      "             26 CALL_FUNCTION            2\n",
      "             28 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,080] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,082] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in func0>\n",
      "[2023-04-04 10:02:49,082] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 14 []\n",
      "[2023-04-04 10:02:49,082] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/3110369299.py:3\n",
      "[2023-04-04 10:02:49,083] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,083] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:49,083] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,085] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:49,085] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in func0> (RETURN_VALUE)\n",
      "[2023-04-04 10:02:49,086] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:49,086] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/3110369299.py, line 3 in <resume in func0>>])\n",
      "[2023-04-04 10:02:49,087] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,094] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11\n",
      "[2023-04-04 10:02:49,100] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/wp/cwp4j23ojvzq6bjw3jiiilkes5m6eiy2hm5pk4bvryqtmuzuq2z4.py\n",
      "[2023-04-04 10:02:49,100] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11\n",
      "[2023-04-04 10:02:49,101] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,101] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_18 <eval_with_key>.80 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/3110369299.py:3, code: return a + b\n",
      "        add = a + b;  a = b = None\n",
      "        return (add,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,102] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_18 <eval_with_key>.80 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  add     <built-in function add>  (a, b)     {}\n",
      "output         output  output                   ((add,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:49,102] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE <resume in func0> /tmp/ipykernel_468424/3110369299.py line 2 \n",
      "  2           0 JUMP_ABSOLUTE           14\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_ATTR                0 (sum)\n",
      "              6 CALL_FUNCTION            0\n",
      "              8 LOAD_CONST               1 (0)\n",
      "             10 COMPARE_OP               4 (>)\n",
      "             12 POP_JUMP_IF_FALSE       22\n",
      "\n",
      "  3     >>   14 LOAD_FAST                0 (a)\n",
      "             16 LOAD_FAST                1 (b)\n",
      "             18 BINARY_ADD\n",
      "             20 RETURN_VALUE\n",
      "\n",
      "  4     >>   22 LOAD_FAST                0 (a)\n",
      "             24 LOAD_FAST                1 (b)\n",
      "             26 BINARY_SUBTRACT\n",
      "             28 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,103] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE <resume in func0> /tmp/ipykernel_468424/3110369299.py line 2 \n",
      "  2           0 LOAD_GLOBAL              1 (__compiled_fn_18)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,103] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,105] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in func1>\n",
      "[2023-04-04 10:02:49,105] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []\n",
      "[2023-04-04 10:02:49,106] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack1 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,106] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 24 [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,107] torch._dynamo.symbolic_convert: [DEBUG] TRACE INPLACE_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,108] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST res [TensorVariable()]\n",
      "[2023-04-04 10:02:49,108] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/3110369299.py:9\n",
      "[2023-04-04 10:02:49,109] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST res []\n",
      "[2023-04-04 10:02:49,109] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:49,109] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in func1> (RETURN_VALUE)\n",
      "[2023-04-04 10:02:49,110] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:49,110] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/3110369299.py, line 9 in <resume in func1>>])\n",
      "[2023-04-04 10:02:49,111] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,119] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12\n",
      "[2023-04-04 10:02:49,147] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/u5/cu5gvf7zwhqqxbb5xgaedh5zj4himh6pnawcvqjzvwytf6etjyxi.py\n",
      "[2023-04-04 10:02:49,148] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12\n",
      "[2023-04-04 10:02:49,149] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,150] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_19 <eval_with_key>.87 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, _stack0 : torch.Tensor, _stack1 : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/3110369299.py:8, code: res += func0(a, b)\n",
      "        _stack0 += _stack1;  iadd = _stack0;  _stack0 = _stack1 = None\n",
      "        return (iadd,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,150] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_19 <eval_with_key>.87 opcode         name     target                    args                kwargs\n",
      "-------------  -------  ------------------------  ------------------  --------\n",
      "placeholder    _stack0  _stack0                   ()                  {}\n",
      "placeholder    _stack1  _stack1                   ()                  {}\n",
      "call_function  iadd     <built-in function iadd>  (_stack0, _stack1)  {}\n",
      "output         output   output                    ((iadd,),)          {}\n",
      "\n",
      "[2023-04-04 10:02:49,151] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE <resume in func1> /tmp/ipykernel_468424/3110369299.py line 8 \n",
      "  8           0 LOAD_FAST                0 (___stack0)\n",
      "              2 LOAD_FAST                1 (___stack1)\n",
      "              4 JUMP_ABSOLUTE           24\n",
      "              6 LOAD_FAST                2 (a)\n",
      "              8 LOAD_FAST                3 (b)\n",
      "             10 BINARY_SUBTRACT\n",
      "             12 STORE_FAST               4 (res)\n",
      "             14 LOAD_FAST                4 (res)\n",
      "             16 LOAD_GLOBAL              0 (func0)\n",
      "             18 LOAD_FAST                2 (a)\n",
      "             20 LOAD_FAST                3 (b)\n",
      "             22 CALL_FUNCTION            2\n",
      "        >>   24 INPLACE_ADD\n",
      "             26 STORE_FAST               4 (res)\n",
      "\n",
      "  9          28 LOAD_FAST                4 (res)\n",
      "             30 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,152] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE <resume in func1> /tmp/ipykernel_468424/3110369299.py line 8 \n",
      "  8           0 LOAD_GLOBAL              1 (__compiled_fn_19)\n",
      "              2 LOAD_FAST                0 (___stack0)\n",
      "              4 LOAD_FAST                1 (___stack1)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,152] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local '___stack0' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local '___stack1' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,154] torch._dynamo.eval_frame: [DEBUG] skipping __call__ /usr/lib/python3.8/weakref.py\n",
      "[2023-04-04 10:02:49,154] torch._dynamo.eval_frame: [DEBUG] skipping del_ten /home/chunwei/newenv2/lib/python3.8/site-packages/torch/_subclasses/meta_utils.py\n",
      "[2023-04-04 10:02:49,154] torch._dynamo.eval_frame: [DEBUG] skipping pop /usr/lib/python3.8/weakref.py\n",
      "[2023-04-04 10:02:49,155] torch._dynamo.eval_frame: [DEBUG] skipping __hash__ /home/chunwei/newenv2/lib/python3.8/site-packages/torch/utils/weak.py\n",
      "[2023-04-04 10:02:49,155] torch._dynamo.eval_frame: [DEBUG] skipping expired /home/chunwei/newenv2/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\n",
      "[2023-04-04 10:02:49,156] torch._dynamo.eval_frame: [DEBUG] skipping _expired /home/chunwei/newenv2/lib/python3.8/site-packages/torch/storage.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[19.4847,  8.2257,  8.0096, 13.1550],\n",
       "        [18.8481,  1.9462,  9.3779,  1.1318],\n",
       "        [18.2816,  7.6292,  2.4458, 10.7018]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func0(a, b):\n",
    "    if a.sum() > 0: \n",
    "        return a + b\n",
    "    return a - b\n",
    "\n",
    "def func1(a, b):\n",
    "    res = a - b\n",
    "    res += func0(a, b)\n",
    "    return res\n",
    "\n",
    "fn = torch.compile(func1)\n",
    "\n",
    "fn(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2. the recursive call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-04 10:02:49,203] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func2\n",
      "[2023-04-04 10:02:49,203] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:12\n",
      "[2023-04-04 10:02:49,204] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,204] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:49,204] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBTRACT None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,206] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST res [TensorVariable()]\n",
      "[2023-04-04 10:02:49,207] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:13\n",
      "[2023-04-04 10:02:49,207] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST res []\n",
      "[2023-04-04 10:02:49,208] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL func1 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,208] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [TensorVariable(), UserFunctionVariable()]\n",
      "[2023-04-04 10:02:49,208] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable(), UserFunctionVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,209] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [TensorVariable(), UserFunctionVariable(), TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,209] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object func1 at 0x7f760b0e4240, file \"/tmp/ipykernel_468424/722712950.py\", line 6>\n",
      "  7           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_FAST                1 (b)\n",
      "              4 BINARY_SUBTRACT\n",
      "              6 STORE_FAST               2 (res)\n",
      "\n",
      "  8           8 LOAD_FAST                2 (res)\n",
      "             10 LOAD_GLOBAL              0 (func0)\n",
      "             12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 CALL_FUNCTION            2\n",
      "             18 INPLACE_ADD\n",
      "             20 STORE_FAST               2 (res)\n",
      "\n",
      "  9          22 LOAD_FAST                2 (res)\n",
      "             24 RETURN_VALUE\n",
      "\n",
      "[2023-04-04 10:02:49,210] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:7\n",
      "[2023-04-04 10:02:49,210] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,211] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:49,211] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBTRACT None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,213] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST res [TensorVariable()]\n",
      "[2023-04-04 10:02:49,213] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:8\n",
      "[2023-04-04 10:02:49,214] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST res []\n",
      "[2023-04-04 10:02:49,214] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL func0 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,214] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [TensorVariable(), UserFunctionVariable()]\n",
      "[2023-04-04 10:02:49,215] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable(), UserFunctionVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,215] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [TensorVariable(), UserFunctionVariable(), TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,216] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object func0 at 0x7f7606e83660, file \"/tmp/ipykernel_468424/722712950.py\", line 1>\n",
      "  2           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_METHOD              0 (sum)\n",
      "              4 CALL_METHOD              0\n",
      "              6 LOAD_CONST               1 (0)\n",
      "              8 COMPARE_OP               4 (>)\n",
      "             10 POP_JUMP_IF_FALSE       20\n",
      "\n",
      "  3          12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 BINARY_ADD\n",
      "             18 RETURN_VALUE\n",
      "\n",
      "  4     >>   20 LOAD_FAST                0 (a)\n",
      "             22 LOAD_FAST                1 (b)\n",
      "             24 BINARY_SUBTRACT\n",
      "             26 RETURN_VALUE\n",
      "\n",
      "[2023-04-04 10:02:49,217] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:2\n",
      "[2023-04-04 10:02:49,217] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,217] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]\n",
      "[2023-04-04 10:02:49,218] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]\n",
      "[2023-04-04 10:02:49,220] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,221] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP > [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:49,222] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 20 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,223] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object func0 at 0x7f7606e83660, file \"/tmp/ipykernel_468424/722712950.py\", line 1>\n",
      "[2023-04-04 10:02:49,224] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 2 nodes\n",
      "[2023-04-04 10:02:49,224] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object func1 at 0x7f760b0e4240, file \"/tmp/ipykernel_468424/722712950.py\", line 6>\n",
      "[2023-04-04 10:02:49,225] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 1 nodes\n",
      "[2023-04-04 10:02:49,225] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 347, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1003, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 495, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/variables/functions.py\", line 265, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/variables/functions.py\", line 98, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 531, in inline_user_function_return\n",
      "    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1947, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 2011, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 604, in run\n",
      "    and self.step()\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 564, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 347, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1003, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 495, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/variables/functions.py\", line 265, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/variables/functions.py\", line 98, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 531, in inline_user_function_return\n",
      "    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1947, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 2011, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 604, in run\n",
      "    and self.step()\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 564, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 332, in inner\n",
      "    unimplemented(f\"generic_jump {typestr(value)}\")\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/exc.py\", line 78, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: generic_jump TensorVariable()\n",
      "[2023-04-04 10:02:49,226] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes\n",
      "[2023-04-04 10:02:49,226] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file /tmp/ipykernel_468424/722712950.py, line 13 in func2>, <FrameSummary file /tmp/ipykernel_468424/722712950.py, line 8 in func1>, <FrameSummary file /tmp/ipykernel_468424/722712950.py, line 2 in func0>])\n",
      "[2023-04-04 10:02:49,227] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13\n",
      "[2023-04-04 10:02:49,240] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/jh/cjhsnrywhg5wzmv4uqfxt544p35eeaaesx4veww7uzyxb5umcdeq.py\n",
      "[2023-04-04 10:02:49,241] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13\n",
      "[2023-04-04 10:02:49,242] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,242] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_20 <eval_with_key>.94 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/722712950.py:12, code: res = a - b\n",
      "        sub = a - b;  a = b = None\n",
      "        return (sub,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,243] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_20 <eval_with_key>.94 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  sub     <built-in function sub>  (a, b)     {}\n",
      "output         output  output                   ((sub,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:49,244] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE func2 /tmp/ipykernel_468424/722712950.py line 11 \n",
      " 12           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_FAST                1 (b)\n",
      "              4 BINARY_SUBTRACT\n",
      "              6 STORE_FAST               2 (res)\n",
      "\n",
      " 13           8 LOAD_FAST                2 (res)\n",
      "             10 LOAD_GLOBAL              0 (func1)\n",
      "             12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 CALL_FUNCTION            2\n",
      "             18 INPLACE_ADD\n",
      "             20 STORE_FAST               2 (res)\n",
      "\n",
      " 14          22 LOAD_FAST                2 (res)\n",
      "             24 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,244] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE func2 /tmp/ipykernel_468424/722712950.py line 11 \n",
      " 11           0 LOAD_GLOBAL              1 (__compiled_fn_20)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 STORE_FAST               3 (___graph_out_0)\n",
      "             10 LOAD_FAST                3 (___graph_out_0)\n",
      "             12 LOAD_CONST               1 (0)\n",
      "             14 BINARY_SUBSCR\n",
      "             16 LOAD_GLOBAL              0 (func1)\n",
      "             18 LOAD_FAST                0 (a)\n",
      "             20 LOAD_FAST                1 (b)\n",
      "\n",
      " 13          22 CALL_FUNCTION            2\n",
      "             24 LOAD_GLOBAL              2 (__resume_at_18_21)\n",
      "             26 ROT_THREE\n",
      "             28 CALL_FUNCTION            2\n",
      "             30 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,245] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'func1' FUNCTION_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,247] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func1\n",
      "[2023-04-04 10:02:49,247] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:7\n",
      "[2023-04-04 10:02:49,248] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,248] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:49,248] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBTRACT None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,250] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST res [TensorVariable()]\n",
      "[2023-04-04 10:02:49,251] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:8\n",
      "[2023-04-04 10:02:49,251] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST res []\n",
      "[2023-04-04 10:02:49,251] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL func0 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,252] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a [TensorVariable(), UserFunctionVariable()]\n",
      "[2023-04-04 10:02:49,252] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable(), UserFunctionVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,253] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 2 [TensorVariable(), UserFunctionVariable(), TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,253] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object func0 at 0x7f7606e83660, file \"/tmp/ipykernel_468424/722712950.py\", line 1>\n",
      "  2           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_METHOD              0 (sum)\n",
      "              4 CALL_METHOD              0\n",
      "              6 LOAD_CONST               1 (0)\n",
      "              8 COMPARE_OP               4 (>)\n",
      "             10 POP_JUMP_IF_FALSE       20\n",
      "\n",
      "  3          12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 BINARY_ADD\n",
      "             18 RETURN_VALUE\n",
      "\n",
      "  4     >>   20 LOAD_FAST                0 (a)\n",
      "             22 LOAD_FAST                1 (b)\n",
      "             24 BINARY_SUBTRACT\n",
      "             26 RETURN_VALUE\n",
      "\n",
      "[2023-04-04 10:02:49,254] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:2\n",
      "[2023-04-04 10:02:49,254] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,255] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]\n",
      "[2023-04-04 10:02:49,255] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]\n",
      "[2023-04-04 10:02:49,258] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,258] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP > [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:49,260] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 20 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,260] torch._dynamo.symbolic_convert: [DEBUG] FAILED INLINING <code object func0 at 0x7f7606e83660, file \"/tmp/ipykernel_468424/722712950.py\", line 1>\n",
      "[2023-04-04 10:02:49,261] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 2 nodes\n",
      "[2023-04-04 10:02:49,261] torch._dynamo.symbolic_convert: [DEBUG] break_graph_if_unsupported triggered compile\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 347, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1003, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 495, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/variables/functions.py\", line 265, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/variables/functions.py\", line 98, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 531, in inline_user_function_return\n",
      "    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1947, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 2011, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 604, in run\n",
      "    and self.step()\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 564, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 332, in inner\n",
      "    unimplemented(f\"generic_jump {typestr(value)}\")\n",
      "  File \"/home/chunwei/newenv2/lib/python3.8/site-packages/torch/_dynamo/exc.py\", line 78, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: generic_jump TensorVariable()\n",
      "[2023-04-04 10:02:49,262] torch._dynamo.output_graph: [DEBUG] restore_graphstate: removed 0 nodes\n",
      "[2023-04-04 10:02:49,262] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file /tmp/ipykernel_468424/722712950.py, line 8 in func1>, <FrameSummary file /tmp/ipykernel_468424/722712950.py, line 2 in func0>])\n",
      "[2023-04-04 10:02:49,263] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,271] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14\n",
      "[2023-04-04 10:02:49,277] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/jh/cjhsnrywhg5wzmv4uqfxt544p35eeaaesx4veww7uzyxb5umcdeq.py\n",
      "[2023-04-04 10:02:49,277] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14\n",
      "[2023-04-04 10:02:49,278] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,279] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_22 <eval_with_key>.101 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/722712950.py:7, code: res = a - b\n",
      "        sub = a - b;  a = b = None\n",
      "        return (sub,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,279] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_22 <eval_with_key>.101 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  sub     <built-in function sub>  (a, b)     {}\n",
      "output         output  output                   ((sub,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:49,280] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE func1 /tmp/ipykernel_468424/722712950.py line 6 \n",
      "  7           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_FAST                1 (b)\n",
      "              4 BINARY_SUBTRACT\n",
      "              6 STORE_FAST               2 (res)\n",
      "\n",
      "  8           8 LOAD_FAST                2 (res)\n",
      "             10 LOAD_GLOBAL              0 (func0)\n",
      "             12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 CALL_FUNCTION            2\n",
      "             18 INPLACE_ADD\n",
      "             20 STORE_FAST               2 (res)\n",
      "\n",
      "  9          22 LOAD_FAST                2 (res)\n",
      "             24 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,281] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE func1 /tmp/ipykernel_468424/722712950.py line 6 \n",
      "  6           0 LOAD_GLOBAL              1 (__compiled_fn_22)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 STORE_FAST               3 (___graph_out_0)\n",
      "             10 LOAD_FAST                3 (___graph_out_0)\n",
      "             12 LOAD_CONST               1 (0)\n",
      "             14 BINARY_SUBSCR\n",
      "             16 LOAD_GLOBAL              0 (func0)\n",
      "             18 LOAD_FAST                0 (a)\n",
      "             20 LOAD_FAST                1 (b)\n",
      "\n",
      "  8          22 CALL_FUNCTION            2\n",
      "             24 LOAD_GLOBAL              2 (__resume_at_18_23)\n",
      "             26 ROT_THREE\n",
      "             28 CALL_FUNCTION            2\n",
      "             30 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,282] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            global 'func0' FUNCTION_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,284] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing func0\n",
      "[2023-04-04 10:02:49,284] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:2\n",
      "[2023-04-04 10:02:49,285] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,285] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR sum [TensorVariable()]\n",
      "[2023-04-04 10:02:49,286] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 0 [GetAttrVariable(TensorVariable(), sum)]\n",
      "[2023-04-04 10:02:49,288] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,288] torch._dynamo.symbolic_convert: [DEBUG] TRACE COMPARE_OP > [TensorVariable(), ConstantVariable(int)]\n",
      "[2023-04-04 10:02:49,289] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_JUMP_IF_FALSE 20 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,290] torch._dynamo.symbolic_convert: [DEBUG] generic_jump triggered compile\n",
      "[2023-04-04 10:02:49,290] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='generic_jump TensorVariable()', user_stack=[<FrameSummary file /tmp/ipykernel_468424/722712950.py, line 2 in func0>])\n",
      "[2023-04-04 10:02:49,291] torch._dynamo.output_graph: [DEBUG] REMOVE UNUSED GRAPHARG b\n",
      "[2023-04-04 10:02:49,292] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,300] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15\n",
      "[2023-04-04 10:02:49,306] torch._inductor.scheduler: [DEBUG] remove_buffer('buf0')\n",
      "[2023-04-04 10:02:49,307] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/6n/c6n4gcygqbe6jhr5lxiggwurqjiyaewzuu3l7wcidyuf4wl6r77v.py\n",
      "[2023-04-04 10:02:49,307] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15\n",
      "[2023-04-04 10:02:49,308] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,309] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_24 <eval_with_key>.108 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/722712950.py:2, code: if a.sum() > 0:\n",
      "        sum_1 = a.sum();  a = None\n",
      "        gt = sum_1 > 0;  sum_1 = None\n",
      "        return (gt,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,309] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_24 <eval_with_key>.108 opcode         name    target                  args        kwargs\n",
      "-------------  ------  ----------------------  ----------  --------\n",
      "placeholder    a       a                       ()          {}\n",
      "call_method    sum_1   sum                     (a,)        {}\n",
      "call_function  gt      <built-in function gt>  (sum_1, 0)  {}\n",
      "output         output  output                  ((gt,),)    {}\n",
      "\n",
      "[2023-04-04 10:02:49,311] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE func0 /tmp/ipykernel_468424/722712950.py line 1 \n",
      "  2           0 LOAD_FAST                0 (a)\n",
      "              2 LOAD_METHOD              0 (sum)\n",
      "              4 CALL_METHOD              0\n",
      "              6 LOAD_CONST               1 (0)\n",
      "              8 COMPARE_OP               4 (>)\n",
      "             10 POP_JUMP_IF_FALSE       20\n",
      "\n",
      "  3          12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 BINARY_ADD\n",
      "             18 RETURN_VALUE\n",
      "\n",
      "  4     >>   20 LOAD_FAST                0 (a)\n",
      "             22 LOAD_FAST                1 (b)\n",
      "             24 BINARY_SUBTRACT\n",
      "             26 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,311] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE func0 /tmp/ipykernel_468424/722712950.py line 1 \n",
      "  1           0 LOAD_GLOBAL              1 (__compiled_fn_24)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 CALL_FUNCTION            1\n",
      "              6 UNPACK_SEQUENCE          1\n",
      "              8 POP_JUMP_IF_FALSE       20\n",
      "             10 LOAD_GLOBAL              2 (__resume_at_12_25)\n",
      "             12 LOAD_FAST                0 (a)\n",
      "             14 LOAD_FAST                1 (b)\n",
      "             16 CALL_FUNCTION            2\n",
      "             18 RETURN_VALUE\n",
      "        >>   20 LOAD_GLOBAL              3 (__resume_at_20_26)\n",
      "             22 LOAD_FAST                0 (a)\n",
      "             24 LOAD_FAST                1 (b)\n",
      "             26 CALL_FUNCTION            2\n",
      "             28 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,312] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,315] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in func0>\n",
      "[2023-04-04 10:02:49,315] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 14 []\n",
      "[2023-04-04 10:02:49,316] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:3\n",
      "[2023-04-04 10:02:49,316] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST a []\n",
      "[2023-04-04 10:02:49,316] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST b [TensorVariable()]\n",
      "[2023-04-04 10:02:49,317] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,318] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:49,319] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in func0> (RETURN_VALUE)\n",
      "[2023-04-04 10:02:49,319] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:49,319] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/722712950.py, line 3 in <resume in func0>>])\n",
      "[2023-04-04 10:02:49,321] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,327] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16\n",
      "[2023-04-04 10:02:49,332] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/wp/cwp4j23ojvzq6bjw3jiiilkes5m6eiy2hm5pk4bvryqtmuzuq2z4.py\n",
      "[2023-04-04 10:02:49,333] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16\n",
      "[2023-04-04 10:02:49,334] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,334] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_27 <eval_with_key>.115 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/722712950.py:3, code: return a + b\n",
      "        add = a + b;  a = b = None\n",
      "        return (add,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,335] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_27 <eval_with_key>.115 opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    a       a                        ()         {}\n",
      "placeholder    b       b                        ()         {}\n",
      "call_function  add     <built-in function add>  (a, b)     {}\n",
      "output         output  output                   ((add,),)  {}\n",
      "\n",
      "[2023-04-04 10:02:49,335] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE <resume in func0> /tmp/ipykernel_468424/722712950.py line 2 \n",
      "  2           0 JUMP_ABSOLUTE           14\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_ATTR                0 (sum)\n",
      "              6 CALL_FUNCTION            0\n",
      "              8 LOAD_CONST               1 (0)\n",
      "             10 COMPARE_OP               4 (>)\n",
      "             12 POP_JUMP_IF_FALSE       22\n",
      "\n",
      "  3     >>   14 LOAD_FAST                0 (a)\n",
      "             16 LOAD_FAST                1 (b)\n",
      "             18 BINARY_ADD\n",
      "             20 RETURN_VALUE\n",
      "\n",
      "  4     >>   22 LOAD_FAST                0 (a)\n",
      "             24 LOAD_FAST                1 (b)\n",
      "             26 BINARY_SUBTRACT\n",
      "             28 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,336] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE <resume in func0> /tmp/ipykernel_468424/722712950.py line 2 \n",
      "  2           0 LOAD_GLOBAL              1 (__compiled_fn_27)\n",
      "              2 LOAD_FAST                0 (a)\n",
      "              4 LOAD_FAST                1 (b)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,336] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local 'a' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local 'b' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,338] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in func1>\n",
      "[2023-04-04 10:02:49,339] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []\n",
      "[2023-04-04 10:02:49,340] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack1 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,340] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 24 [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,340] torch._dynamo.symbolic_convert: [DEBUG] TRACE INPLACE_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,342] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST res [TensorVariable()]\n",
      "[2023-04-04 10:02:49,342] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:9\n",
      "[2023-04-04 10:02:49,343] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST res []\n",
      "[2023-04-04 10:02:49,343] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:49,344] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in func1> (RETURN_VALUE)\n",
      "[2023-04-04 10:02:49,344] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:49,344] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/722712950.py, line 9 in <resume in func1>>])\n",
      "[2023-04-04 10:02:49,345] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,353] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17\n",
      "[2023-04-04 10:02:49,361] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/u5/cu5gvf7zwhqqxbb5xgaedh5zj4himh6pnawcvqjzvwytf6etjyxi.py\n",
      "[2023-04-04 10:02:49,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17\n",
      "[2023-04-04 10:02:49,363] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,363] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_28 <eval_with_key>.122 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, _stack0 : torch.Tensor, _stack1 : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/722712950.py:8, code: res += func0(a, b)\n",
      "        _stack0 += _stack1;  iadd = _stack0;  _stack0 = _stack1 = None\n",
      "        return (iadd,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,364] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_28 <eval_with_key>.122 opcode         name     target                    args                kwargs\n",
      "-------------  -------  ------------------------  ------------------  --------\n",
      "placeholder    _stack0  _stack0                   ()                  {}\n",
      "placeholder    _stack1  _stack1                   ()                  {}\n",
      "call_function  iadd     <built-in function iadd>  (_stack0, _stack1)  {}\n",
      "output         output   output                    ((iadd,),)          {}\n",
      "\n",
      "[2023-04-04 10:02:49,365] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE <resume in func1> /tmp/ipykernel_468424/722712950.py line 8 \n",
      "  8           0 LOAD_FAST                0 (___stack0)\n",
      "              2 LOAD_FAST                1 (___stack1)\n",
      "              4 JUMP_ABSOLUTE           24\n",
      "              6 LOAD_FAST                2 (a)\n",
      "              8 LOAD_FAST                3 (b)\n",
      "             10 BINARY_SUBTRACT\n",
      "             12 STORE_FAST               4 (res)\n",
      "             14 LOAD_FAST                4 (res)\n",
      "             16 LOAD_GLOBAL              0 (func0)\n",
      "             18 LOAD_FAST                2 (a)\n",
      "             20 LOAD_FAST                3 (b)\n",
      "             22 CALL_FUNCTION            2\n",
      "        >>   24 INPLACE_ADD\n",
      "             26 STORE_FAST               4 (res)\n",
      "\n",
      "  9          28 LOAD_FAST                4 (res)\n",
      "             30 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,365] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE <resume in func1> /tmp/ipykernel_468424/722712950.py line 8 \n",
      "  8           0 LOAD_GLOBAL              1 (__compiled_fn_28)\n",
      "              2 LOAD_FAST                0 (___stack0)\n",
      "              4 LOAD_FAST                1 (___stack1)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,366] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local '___stack0' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local '___stack1' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      "[2023-04-04 10:02:49,368] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <resume in func2>\n",
      "[2023-04-04 10:02:49,368] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack0 []\n",
      "[2023-04-04 10:02:49,369] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST ___stack1 [TensorVariable()]\n",
      "[2023-04-04 10:02:49,369] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_ABSOLUTE 24 [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,369] torch._dynamo.symbolic_convert: [DEBUG] TRACE INPLACE_ADD None [TensorVariable(), TensorVariable()]\n",
      "[2023-04-04 10:02:49,371] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST res [TensorVariable()]\n",
      "[2023-04-04 10:02:49,371] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_468424/722712950.py:14\n",
      "[2023-04-04 10:02:49,372] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST res []\n",
      "[2023-04-04 10:02:49,372] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-04-04 10:02:49,372] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <resume in func2> (RETURN_VALUE)\n",
      "[2023-04-04 10:02:49,373] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-04-04 10:02:49,373] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_468424/722712950.py, line 14 in <resume in func2>>])\n",
      "[2023-04-04 10:02:49,374] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,383] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18\n",
      "[2023-04-04 10:02:49,391] torch._inductor.graph: [DEBUG] Output code written to: /tmp/torchinductor_chunwei/u5/cu5gvf7zwhqqxbb5xgaedh5zj4himh6pnawcvqjzvwytf6etjyxi.py\n",
      "[2023-04-04 10:02:49,391] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18\n",
      "[2023-04-04 10:02:49,392] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
      "[2023-04-04 10:02:49,392] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_29 <eval_with_key>.129 class GraphModule(torch.nn.Module):\n",
      "    def forward(self, _stack0 : torch.Tensor, _stack1 : torch.Tensor):\n",
      "        # File: /tmp/ipykernel_468424/722712950.py:13, code: res += func1(a, b)\n",
      "        _stack0 += _stack1;  iadd = _stack0;  _stack0 = _stack1 = None\n",
      "        return (iadd,)\n",
      "        \n",
      "\n",
      "[2023-04-04 10:02:49,393] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      " __compiled_fn_29 <eval_with_key>.129 opcode         name     target                    args                kwargs\n",
      "-------------  -------  ------------------------  ------------------  --------\n",
      "placeholder    _stack0  _stack0                   ()                  {}\n",
      "placeholder    _stack1  _stack1                   ()                  {}\n",
      "call_function  iadd     <built-in function iadd>  (_stack0, _stack1)  {}\n",
      "output         output   output                    ((iadd,),)          {}\n",
      "\n",
      "[2023-04-04 10:02:49,393] torch._dynamo.convert_frame.__bytecode: [DEBUG] ORIGINAL BYTECODE <resume in func2> /tmp/ipykernel_468424/722712950.py line 13 \n",
      " 13           0 LOAD_FAST                0 (___stack0)\n",
      "              2 LOAD_FAST                1 (___stack1)\n",
      "              4 JUMP_ABSOLUTE           24\n",
      "              6 LOAD_FAST                2 (a)\n",
      "              8 LOAD_FAST                3 (b)\n",
      "             10 BINARY_SUBTRACT\n",
      "             12 STORE_FAST               4 (res)\n",
      "             14 LOAD_FAST                4 (res)\n",
      "             16 LOAD_GLOBAL              0 (func1)\n",
      "             18 LOAD_FAST                2 (a)\n",
      "             20 LOAD_FAST                3 (b)\n",
      "             22 CALL_FUNCTION            2\n",
      "        >>   24 INPLACE_ADD\n",
      "             26 STORE_FAST               4 (res)\n",
      "\n",
      " 14          28 LOAD_FAST                4 (res)\n",
      "             30 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,394] torch._dynamo.convert_frame.__bytecode: [DEBUG] MODIFIED BYTECODE <resume in func2> /tmp/ipykernel_468424/722712950.py line 13 \n",
      " 13           0 LOAD_GLOBAL              1 (__compiled_fn_29)\n",
      "              2 LOAD_FAST                0 (___stack0)\n",
      "              4 LOAD_FAST                1 (___stack1)\n",
      "              6 CALL_FUNCTION            2\n",
      "              8 UNPACK_SEQUENCE          1\n",
      "             10 RETURN_VALUE\n",
      "\n",
      "\n",
      "[2023-04-04 10:02:49,395] torch._dynamo.convert_frame.__guards: [DEBUG] GUARDS:\n",
      " - \n",
      "            local '___stack0' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n",
      " - \n",
      "            local '___stack1' TENSOR_MATCH\n",
      "            {\n",
      "                'guard_types': None,\n",
      "                'code': None,\n",
      "                'obj_weakref': None\n",
      "                'guarded_class': None\n",
      "            }\n",
      "            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[27.4968, 11.6127, 11.2419, 18.5324],\n",
       "        [26.5952,  2.8156, 13.1936,  1.6404],\n",
       "        [25.7192, 10.7857,  3.5256, 15.0578]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func0(a, b):\n",
    "    if a.sum() > 0: \n",
    "        return a + b\n",
    "    return a - b\n",
    "\n",
    "def func1(a, b):\n",
    "    res = a - b\n",
    "    res += func0(a, b)\n",
    "    return res\n",
    "\n",
    "def func2(a, b):\n",
    "    res = a - b\n",
    "    res += func1(a, b)\n",
    "    return res\n",
    "\n",
    "fn = torch.compile(func2)\n",
    "\n",
    "fn(a, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could find `FAILED INLINING <code object func0` and `FAILED INLINING <code object func1` in the log, which means that both the func0 and func1 are failed in INLING."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The innermost graph break will disallow the outer functions to be inlined, thus has a huge impact over the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79b3c643fceffa1ffebbbec8e48bcead3967abe49c79586407a5e0726120fadf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
