{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "208f3970",
      "metadata": {
        "id": "208f3970"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch._dynamo import optimize\n",
        "from typing import *\n",
        "from torch import _dynamo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MfOUK6SV-zUB",
      "metadata": {
        "id": "MfOUK6SV-zUB"
      },
      "source": [
        "All the valid backends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "sGlDbGXy-obW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGlDbGXy-obW",
        "outputId": "166cd045-f20b-4000-c1fc-8be9c986da24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['aot_ts_nvfuser',\n",
              " 'cudagraphs',\n",
              " 'inductor',\n",
              " 'ipex',\n",
              " 'nvprims_nvfuser',\n",
              " 'onnxrt',\n",
              " 'tensorrt',\n",
              " 'tvm']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_dynamo.list_backends()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kl3vIxtD-3_5",
      "metadata": {
        "id": "kl3vIxtD-3_5"
      },
      "source": [
        "# optimizer() usage with inductor as backend"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M96heyoMA-h0",
      "metadata": {
        "id": "M96heyoMA-h0"
      },
      "source": [
        "A naive example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "SyuoQza--7Os",
      "metadata": {
        "id": "SyuoQza--7Os"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    x = F.relu(x)\n",
        "    return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2d4d05e9",
      "metadata": {},
      "source": [
        "With configs we could alter the behavior of both TorchDynamo and TorchInductor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "45WebTphE2dh",
      "metadata": {
        "id": "45WebTphE2dh"
      },
      "outputs": [],
      "source": [
        "from torch._inductor import config as inductor_config\n",
        "from torch._dynamo import config as dynamo_config\n",
        "import logging\n",
        "\n",
        "inductor_config.debug = True\n",
        "dynamo_config.verbose = True\n",
        "dynamo_config.log_level = logging.DEBUG\n",
        "dynamo_config.output_code = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "TIW9R-cGBDJc",
      "metadata": {
        "id": "TIW9R-cGBDJc"
      },
      "outputs": [],
      "source": [
        "foo = Net()\n",
        "foo = torch.compile(foo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "VhuU13T3BL8j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhuU13T3BL8j",
        "outputId": "084bdb35-fd14-4794-f01b-46ac83bc7a38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch._dynamo.eval_frame.OptimizedModule"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "foo\n",
        "type(foo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kok22EXnHh1y",
      "metadata": {
        "id": "kok22EXnHh1y"
      },
      "source": [
        "When enable `inductor.debug`, it could dump the python code it codegened."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "pAvJRtdxD24H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAvJRtdxD24H",
        "outputId": "b0a42b77-2ccd-4507-b8f7-69796e47e0e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-02-28 16:11:48,236] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /usr/lib/python3.8/contextlib.py\n",
            "[2023-02-28 16:11:48,237] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /usr/lib/python3.8/contextlib.py\n",
            "[2023-02-28 16:11:48,237] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /usr/lib/python3.8/contextlib.py\n",
            "[2023-02-28 16:11:48,238] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /usr/lib/python3.8/contextlib.py\n",
            "[2023-02-28 16:11:48,238] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /home/chunwei/trienv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py\n",
            "[2023-02-28 16:11:48,255] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
            "[2023-02-28 16:11:48,256] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_3597807/3534211230.py:11\n",
            "[2023-02-28 16:11:48,256] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
            "[2023-02-28 16:11:48,257] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR fc [NNModuleVariable()]\n",
            "[2023-02-28 16:11:48,257] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x [NNModuleVariable()]\n",
            "[2023-02-28 16:11:48,258] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [NNModuleVariable(), TensorVariable()]\n",
            "[2023-02-28 16:11:48,266] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST x [TensorVariable()]\n",
            "[2023-02-28 16:11:48,266] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_3597807/3534211230.py:12\n",
            "[2023-02-28 16:11:48,266] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL F []\n",
            "[2023-02-28 16:11:48,267] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR relu [TorchVariable(<module 'torch.nn.functional' from '/home/chunwei/trienv/lib/python3.8/site-packages/torch/nn/functional.py'>)]\n",
            "[2023-02-28 16:11:48,267] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x [TorchVariable(<function relu at 0x7fa2c81fc9d0>)]\n",
            "[2023-02-28 16:11:48,268] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL_FUNCTION 1 [TorchVariable(<function relu at 0x7fa2c81fc9d0>), TensorVariable()]\n",
            "[2023-02-28 16:11:48,270] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST x [TensorVariable()]\n",
            "[2023-02-28 16:11:48,270] torch._dynamo.symbolic_convert: [DEBUG] TRACE starts_line /tmp/ipykernel_3597807/3534211230.py:13\n",
            "[2023-02-28 16:11:48,271] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x []\n",
            "[2023-02-28 16:11:48,271] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
            "[2023-02-28 16:11:48,271] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "[2023-02-28 16:11:48,272] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
            "[2023-02-28 16:11:48,272] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_3597807/3534211230.py, line 13 in forward>])\n",
            "[2023-02-28 16:11:48,274] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
            "[2023-02-28 16:11:49,553] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compile_fx_inner:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, primals_1: f32[10, 128], primals_2: f32[10], primals_3: f32[2, 128]):\n",
            "        # File: /tmp/ipykernel_3597807/3534211230.py:11, code: x = self.fc(x)\n",
            "        permute: f32[128, 10] = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None\n",
            "        addmm: f32[2, 10] = torch.ops.aten.addmm.default(primals_2, primals_3, permute);  primals_2 = permute = None\n",
            "        \n",
            "        # File: /tmp/ipykernel_3597807/3534211230.py:12, code: x = F.relu(x)\n",
            "        relu: f32[2, 10] = torch.ops.aten.relu.default(addmm);  addmm = None\n",
            "        le: b8[2, 10] = torch.ops.aten.le.Scalar(relu, 0)\n",
            "        return [relu, primals_3, le]\n",
            "        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-02-28 16:11:52,160] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_chunwei/na/cnanocxvxyalevkulr775i4z4mzgb7pag7gju2ollw75ulnzeqpl.py\n",
            "[2023-02-28 16:11:52,161] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0\n",
            "[2023-02-28 16:11:52,163] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n",
            "[2023-02-28 16:11:52,166] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_0 <eval_with_key>.5 opcode         name     target                             args        kwargs\n",
            "-------------  -------  ---------------------------------  ----------  --------\n",
            "placeholder    x        x                                  ()          {}\n",
            "call_module    self_fc  self_fc                            (x,)        {}\n",
            "call_function  relu     <function relu at 0x7fa2c81fc9d0>  (self_fc,)  {}\n",
            "output         output   output                             ((relu,),)  {}\n",
            "\n",
            "[2023-02-28 16:11:52,167] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /tmp/ipykernel_3597807/3534211230.py line 10 \n",
            " 11           0 LOAD_FAST                0 (self)\n",
            "              2 LOAD_METHOD              0 (fc)\n",
            "              4 LOAD_FAST                1 (x)\n",
            "              6 CALL_METHOD              1\n",
            "              8 STORE_FAST               1 (x)\n",
            "\n",
            " 12          10 LOAD_GLOBAL              1 (F)\n",
            "             12 LOAD_METHOD              2 (relu)\n",
            "             14 LOAD_FAST                1 (x)\n",
            "             16 CALL_METHOD              1\n",
            "             18 STORE_FAST               1 (x)\n",
            "\n",
            " 13          20 LOAD_FAST                1 (x)\n",
            "             22 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:11:52,167] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /tmp/ipykernel_3597807/3534211230.py line 10 \n",
            " 10           0 LOAD_GLOBAL              3 (__compiled_fn_0)\n",
            "              2 LOAD_FAST                1 (x)\n",
            "              4 CALL_FUNCTION            1\n",
            "              6 UNPACK_SEQUENCE          1\n",
            "              8 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:11:52,258] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'x' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338d31e00; to 'Tensor' at 0x7fa2c6af23b0>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            " - \n",
            "            local 'self' NN_MODULE\n",
            "            {\n",
            "                'guard_types': ['ID_MATCH'],\n",
            "                'code': ['___check_obj_id(self, 140337078128016)'],\n",
            "                'obj_weakref': <weakref at 0x7fa338c4c540; to 'Net' at 0x7fa2c5b0cd90>\n",
            "                'guarded_class': <weakref at 0x7fa2c6613810; to 'type' at 0x7d2d7c0 (Net)>\n",
            "            }\n",
            "            \n",
            " - \n",
            "            global 'F' FUNCTION_MATCH\n",
            "            {\n",
            "                'guard_types': None,\n",
            "                'code': None,\n",
            "                'obj_weakref': None\n",
            "                'guarded_class': None\n",
            "            }\n",
            "            \n",
            " - \n",
            "            local_nn_module 'self.fc' NN_MODULE\n",
            "            {\n",
            "                'guard_types': None,\n",
            "                'code': None,\n",
            "                'obj_weakref': None\n",
            "                'guarded_class': None\n",
            "            }\n",
            "            \n",
            "[2023-02-28 16:11:52,260] torch._dynamo.eval_frame: [DEBUG] skipping _fn /home/chunwei/trienv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py\n",
            "[2023-02-28 16:11:52,260] torch._dynamo.eval_frame: [DEBUG] skipping nothing /home/chunwei/trienv/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py\n",
            "[2023-02-28 16:11:52,262] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /usr/lib/python3.8/contextlib.py\n",
            "[2023-02-28 16:11:52,262] torch._dynamo.eval_frame: [DEBUG] skipping __exit__ /usr/lib/python3.8/contextlib.py\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "from ctypes import c_void_p, c_long\n",
            "import torch\n",
            "import math\n",
            "import random\n",
            "from torch import empty_strided, as_strided, device\n",
            "from torch._inductor.codecache import AsyncCompile\n",
            "from torch._inductor.select_algorithm import extern_kernels\n",
            "\n",
            "aten = torch.ops.aten\n",
            "assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
            "async_compile = AsyncCompile()\n",
            "\n",
            "import triton\n",
            "import triton.language as tl\n",
            "from torch._inductor.triton_ops.autotune import grid\n",
            "from torch._C import _cuda_getCurrentRawStream as get_cuda_stream\n",
            "\n",
            "\n",
            "kernel_cpp_0 = async_compile.cpp('''\n",
            "#include \"/tmp/torchinductor_chunwei/zt/cztcl2vp5yqlnhofzpqfficjcxgyict6e3xhfdd7sdbkipp4p44x.h\"\n",
            "extern \"C\" void kernel(float* __restrict__ in_out_ptr0,\n",
            "                       bool* __restrict__ out_ptr0)\n",
            "{\n",
            "    {\n",
            "        #pragma GCC ivdep\n",
            "        for(long i0=0; i0<20; i0+=1)\n",
            "        {\n",
            "            auto tmp0 = in_out_ptr0[i0];\n",
            "            auto tmp1 = tmp0 * (tmp0>0);\n",
            "            auto tmp2 = static_cast<float>(0);\n",
            "            auto tmp3 = tmp1 <= tmp2;\n",
            "            in_out_ptr0[i0] = tmp1;\n",
            "            out_ptr0[i0] = tmp3;\n",
            "        }\n",
            "    }\n",
            "}\n",
            "''')\n",
            "\n",
            "\n",
            "async_compile.wait(globals())\n",
            "del async_compile\n",
            "\n",
            "def call(args):\n",
            "    primals_1, primals_2, primals_3 = args\n",
            "    args.clear()\n",
            "    buf0 = empty_strided((2, 10), (10, 1), device='cpu', dtype=torch.float32)\n",
            "    extern_kernels.addmm(primals_2, primals_3, as_strided(primals_1, (128, 10), (1, 128)), alpha=1, beta=1, out=buf0)\n",
            "    del primals_1\n",
            "    del primals_2\n",
            "    buf1 = buf0; del buf0  # reuse\n",
            "    buf2 = empty_strided((2, 10), (10, 1), device='cpu', dtype=torch.bool)\n",
            "    kernel_cpp_0(c_void_p(buf1.data_ptr()), c_void_p(buf2.data_ptr()))\n",
            "    return (buf1, primals_3, buf2, )\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    from torch._dynamo.testing import rand_strided\n",
            "    from torch._inductor.utils import print_performance\n",
            "    primals_1 = rand_strided((10, 128), (128, 1), device='cpu', dtype=torch.float32)\n",
            "    primals_2 = rand_strided((10, ), (1, ), device='cpu', dtype=torch.float32)\n",
            "    primals_3 = rand_strided((2, 128), (128, 1), device='cpu', dtype=torch.float32)\n",
            "    print_performance(lambda: call([primals_1, primals_2, primals_3]))\n",
            "\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa2c5b32b80>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.1221, 0.0000, 0.0000, 0.6039, 0.0000, 0.0000, 0.2984, 0.7128, 0.0169,\n",
              "         0.0000],\n",
              "        [0.0000, 0.3757, 0.0000, 0.0000, 0.0000, 0.9155, 0.7447, 0.2878, 0.2604,\n",
              "         -0.0000]], grad_fn=<CompiledFunctionBackward>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.randn((2, 128))\n",
        "\n",
        "foo(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2sO0MfPoIPc8",
      "metadata": {
        "id": "2sO0MfPoIPc8"
      },
      "source": [
        "# Dive into dynamo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fx2N_KyVIt2U",
      "metadata": {
        "id": "fx2N_KyVIt2U"
      },
      "source": [
        "According to the definition of `_dynamo.optimize`: \n",
        "\n",
        "```python\n",
        "def optimize(\n",
        "    backend=\"inductor\",\n",
        "    *,\n",
        "    nopython=False,\n",
        "    guard_export_fn=None,\n",
        "    guard_fail_fn=None,\n",
        "    disable=False,\n",
        "    dynamic=False,\n",
        "):\n",
        "```\n",
        "\n",
        "The `backend` argument could be either a `str` or a `callable`.\n",
        "Let's hack it with a custom callable to dump something."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "16d974c3",
      "metadata": {
        "id": "16d974c3"
      },
      "outputs": [],
      "source": [
        "my_graph_id = 0\n",
        "def my_compiler( \n",
        "        gm: torch.fx.GraphModule,\n",
        "        inputs: List[torch.Tensor]):\n",
        "    global my_graph_id\n",
        "    print(f\"my_compiler() called with FX graph-{my_graph_id}:\")\n",
        "    my_graph_id += 1\n",
        "    gm.print_readable()\n",
        "    print()\n",
        "    #print(\"tabular:\")\n",
        "    #gm.graph.print_tabular(); print()\n",
        "    #print(f\"code: {gm.graph.python_code()}\")\n",
        "    return gm.forward  # python callable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WTM6v_bLPc9F",
      "metadata": {
        "id": "WTM6v_bLPc9F"
      },
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "SVObPqF3KLla",
      "metadata": {
        "id": "SVObPqF3KLla"
      },
      "outputs": [],
      "source": [
        "def foo1(a:torch.tensor, b:torch.tensor):\n",
        "  x = a + b\n",
        "  if b.sum() < 0:\n",
        "    x = x * -1\n",
        "  return x \n",
        "\n",
        "foo1_ = optimize(my_compiler)(foo1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RH6jw-pRKfg6",
      "metadata": {
        "id": "RH6jw-pRKfg6"
      },
      "source": [
        "Note that, this kernel contains a `if` `if b.sum() < 0`, since the `b.sum()` is determined by its value(dynamic), so it should break the graph into two cases:\n",
        "\n",
        "The first, when the condition is true:\n",
        "\n",
        "```python\n",
        "x = a + b\n",
        "x = x * -1\n",
        "return x\n",
        "```\n",
        "\n",
        "The second, when the condition is false:\n",
        "\n",
        "```python\n",
        "x = a + b\n",
        "return x\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "_0Gp__rxKYRY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Gp__rxKYRY",
        "outputId": "5cfb88db-322c-409a-f89c-573378fde0f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-02-28 16:40:53,790] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing foo1\n",
            "[2023-02-28 16:40:53,795] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 16:40:53,795] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 16:40:53,796] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_55 <eval_with_key>.62 opcode         name    target                   args          kwargs\n",
            "-------------  ------  -----------------------  ------------  --------\n",
            "placeholder    a       a                        ()            {}\n",
            "placeholder    b       b                        ()            {}\n",
            "call_function  add     <built-in function add>  (a, b)        {}\n",
            "call_method    sum_1   sum                      (b,)          {}\n",
            "call_function  lt      <built-in function lt>   (sum_1, 0)    {}\n",
            "output         output  output                   ((add, lt),)  {}\n",
            "\n",
            "[2023-02-28 16:40:53,797] torch._dynamo.symbolic_convert: [INFO] __resume_at_20_56 function\n",
            "[2023-02-28 16:40:53,798] torch._dynamo.symbolic_convert: [INFO] __resume_at_28_57 function\n",
            "[2023-02-28 16:40:53,798] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE foo1 /tmp/ipykernel_3597807/1265408374.py line 1 \n",
            "  2           0 LOAD_FAST                0 (a)\n",
            "              2 LOAD_FAST                1 (b)\n",
            "              4 BINARY_ADD\n",
            "              6 STORE_FAST               2 (x)\n",
            "\n",
            "  3           8 LOAD_FAST                1 (b)\n",
            "             10 LOAD_METHOD              0 (sum)\n",
            "             12 CALL_METHOD              0\n",
            "             14 LOAD_CONST               1 (0)\n",
            "             16 COMPARE_OP               0 (<)\n",
            "             18 POP_JUMP_IF_FALSE       28\n",
            "\n",
            "  4          20 LOAD_FAST                2 (x)\n",
            "             22 LOAD_CONST               2 (-1)\n",
            "             24 BINARY_MULTIPLY\n",
            "             26 STORE_FAST               2 (x)\n",
            "\n",
            "  5     >>   28 LOAD_FAST                2 (x)\n",
            "             30 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:40:53,799] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE foo1 /tmp/ipykernel_3597807/1265408374.py line 1 \n",
            "  1           0 LOAD_GLOBAL              1 (__compiled_fn_55)\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                1 (b)\n",
            "              6 CALL_FUNCTION            2\n",
            "              8 UNPACK_SEQUENCE          2\n",
            "             10 STORE_FAST               2 (x)\n",
            "             12 POP_JUMP_IF_FALSE       22\n",
            "             14 LOAD_GLOBAL              2 (__resume_at_20_56)\n",
            "             16 LOAD_FAST                2 (x)\n",
            "             18 CALL_FUNCTION            1\n",
            "             20 RETURN_VALUE\n",
            "        >>   22 LOAD_GLOBAL              3 (__resume_at_28_57)\n",
            "             24 LOAD_FAST                2 (x)\n",
            "             26 CALL_FUNCTION            1\n",
            "             28 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:40:53,799] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'a' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338b95900; to 'Tensor' at 0x7fa338dd0090>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            " - \n",
            "            local 'b' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338b95f40; to 'Tensor' at 0x7fa338c01450>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            "[2023-02-28 16:40:53,801] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in foo1>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_compiler() called with FX graph-5:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/1265408374.py:2, code: x = a + b\n",
            "        add = a + b;  a = None\n",
            "        \n",
            "        # File: /tmp/ipykernel_3597807/1265408374.py:3, code: if b.sum() < 0:\n",
            "        sum_1 = b.sum();  b = None\n",
            "        lt = sum_1 < 0;  sum_1 = None\n",
            "        return (add, lt)\n",
            "        \n",
            "\n",
            "__resume_at_20_56:\n",
            "  3           0 JUMP_ABSOLUTE           22\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "\n",
            "  4     >>   22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (x)\n",
            "             32 RETURN_VALUE\n",
            "__resume_at_28_57:\n",
            "  3           0 JUMP_ABSOLUTE           30\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (x)\n",
            "             32 RETURN_VALUE\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa2c5b322b0>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[2., 2., 2.],\n",
              "        [2., 2., 2.]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch._dynamo.reset() # reset all che compilation cache\n",
        "dynamo_config.log_level  = logging.INFO\n",
        "\n",
        "a = torch.ones((2, 3))\n",
        "b = torch.ones((2, 3))\n",
        "\n",
        "# It should tigger both cases of the if-else\n",
        "foo1_(a, b)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "26169501",
      "metadata": {},
      "source": [
        "In the above case, due to both a and b are positive, it should not goto the if-then block.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JyKD7EnHQY4T",
      "metadata": {
        "id": "JyKD7EnHQY4T"
      },
      "source": [
        "In the exaple above, it do break into two graphs, but not from expected:\n",
        "\n",
        "- graph1: the expressions before the if, with the condition computation\n",
        "- graph2: the expressions after the if"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1321f550",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-02-28 16:40:56,050] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in foo1>\n",
            "[2023-02-28 16:40:56,052] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in foo1> (RETURN_VALUE)\n",
            "[2023-02-28 16:40:56,054] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 16:40:56,055] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 16:40:56,055] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_58 <eval_with_key>.64 opcode         name    target                   args       kwargs\n",
            "-------------  ------  -----------------------  ---------  --------\n",
            "placeholder    x       x                        ()         {}\n",
            "call_function  mul     <built-in function mul>  (x, -1)    {}\n",
            "output         output  output                   ((mul,),)  {}\n",
            "\n",
            "[2023-02-28 16:40:56,056] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in foo1> /tmp/ipykernel_3597807/1265408374.py line 3 \n",
            "  3           0 JUMP_ABSOLUTE           22\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "\n",
            "  4     >>   22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (x)\n",
            "             32 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:40:56,056] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in foo1> /tmp/ipykernel_3597807/1265408374.py line 3 \n",
            "  3           0 LOAD_GLOBAL              1 (__compiled_fn_58)\n",
            "              2 LOAD_FAST                0 (x)\n",
            "              4 CALL_FUNCTION            1\n",
            "              6 UNPACK_SEQUENCE          1\n",
            "              8 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:40:56,057] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'x' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338bb4d10; to 'Tensor' at 0x7fa338d51180>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_compiler() called with FX graph-6:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, x : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/1265408374.py:4, code: x = x * -1\n",
            "        mul = x * -1;  x = None\n",
            "        return (mul,)\n",
            "        \n",
            "\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338ba2dc0>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0., -0., -0.],\n",
              "        [-0., -0., -0.]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#torch._dynamo.reset() # reset all che compilation cache\n",
        "foo1_(a, -b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tWRo8thZPiTT",
      "metadata": {
        "id": "tWRo8thZPiTT"
      },
      "source": [
        "## Example 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KZCajhxNXhEd",
      "metadata": {
        "id": "KZCajhxNXhEd"
      },
      "source": [
        "### Execute once case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "24StYyTRPlH_",
      "metadata": {
        "id": "24StYyTRPlH_"
      },
      "outputs": [],
      "source": [
        "def foo2(a:torch.tensor, b:torch.tensor):\n",
        "  x = a + b\n",
        "  if b.sum() < 0:\n",
        "    x = x * -1\n",
        "  if a.sum() < 0:\n",
        "    x = x * -2\n",
        "  x = 2 * x\n",
        "  return x\n",
        "\n",
        "foo2_ = optimize(my_compiler)(foo2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5zKWsiCFP08s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zKWsiCFP08s",
        "outputId": "883c4a8d-e589-4ca5-b7da-fdf5b96d671e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-02-28 17:18:54,598] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing foo2\n",
            "[2023-02-28 17:18:54,603] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 17:18:54,603] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 17:18:54,604] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_106 <eval_with_key>.108 opcode         name    target                   args          kwargs\n",
            "-------------  ------  -----------------------  ------------  --------\n",
            "placeholder    a       a                        ()            {}\n",
            "placeholder    b       b                        ()            {}\n",
            "call_function  add     <built-in function add>  (a, b)        {}\n",
            "call_method    sum_1   sum                      (b,)          {}\n",
            "call_function  lt      <built-in function lt>   (sum_1, 0)    {}\n",
            "output         output  output                   ((add, lt),)  {}\n",
            "\n",
            "[2023-02-28 17:18:54,605] torch._dynamo.symbolic_convert: [INFO] __resume_at_20_107 function\n",
            "[2023-02-28 17:18:54,606] torch._dynamo.symbolic_convert: [INFO] __resume_at_28_108 function\n",
            "[2023-02-28 17:18:54,607] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE foo2 /tmp/ipykernel_3597807/2689548108.py line 1 \n",
            "  2           0 LOAD_FAST                0 (a)\n",
            "              2 LOAD_FAST                1 (b)\n",
            "              4 BINARY_ADD\n",
            "              6 STORE_FAST               2 (x)\n",
            "\n",
            "  3           8 LOAD_FAST                1 (b)\n",
            "             10 LOAD_METHOD              0 (sum)\n",
            "             12 CALL_METHOD              0\n",
            "             14 LOAD_CONST               1 (0)\n",
            "             16 COMPARE_OP               0 (<)\n",
            "             18 POP_JUMP_IF_FALSE       28\n",
            "\n",
            "  4          20 LOAD_FAST                2 (x)\n",
            "             22 LOAD_CONST               2 (-1)\n",
            "             24 BINARY_MULTIPLY\n",
            "             26 STORE_FAST               2 (x)\n",
            "\n",
            "  5     >>   28 LOAD_FAST                0 (a)\n",
            "             30 LOAD_METHOD              0 (sum)\n",
            "             32 CALL_METHOD              0\n",
            "             34 LOAD_CONST               1 (0)\n",
            "             36 COMPARE_OP               0 (<)\n",
            "             38 POP_JUMP_IF_FALSE       48\n",
            "\n",
            "  6          40 LOAD_FAST                2 (x)\n",
            "             42 LOAD_CONST               3 (-2)\n",
            "             44 BINARY_MULTIPLY\n",
            "             46 STORE_FAST               2 (x)\n",
            "\n",
            "  7     >>   48 LOAD_CONST               4 (2)\n",
            "             50 LOAD_FAST                2 (x)\n",
            "             52 BINARY_MULTIPLY\n",
            "             54 STORE_FAST               2 (x)\n",
            "\n",
            "  8          56 LOAD_FAST                2 (x)\n",
            "             58 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:18:54,607] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE foo2 /tmp/ipykernel_3597807/2689548108.py line 1 \n",
            "  1           0 LOAD_GLOBAL              1 (__compiled_fn_106)\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                1 (b)\n",
            "              6 CALL_FUNCTION            2\n",
            "              8 UNPACK_SEQUENCE          2\n",
            "             10 STORE_FAST               2 (x)\n",
            "             12 POP_JUMP_IF_FALSE       24\n",
            "             14 LOAD_GLOBAL              2 (__resume_at_20_107)\n",
            "             16 LOAD_FAST                0 (a)\n",
            "             18 LOAD_FAST                2 (x)\n",
            "             20 CALL_FUNCTION            2\n",
            "             22 RETURN_VALUE\n",
            "        >>   24 LOAD_GLOBAL              3 (__resume_at_28_108)\n",
            "             26 LOAD_FAST                0 (a)\n",
            "             28 LOAD_FAST                2 (x)\n",
            "             30 CALL_FUNCTION            2\n",
            "             32 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:18:54,608] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'a' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338e0d1d0; to 'Tensor' at 0x7fa338d92e00>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            " - \n",
            "            local 'b' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338e0d630; to 'Tensor' at 0x7fa33b38a770>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            "[2023-02-28 17:18:54,609] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in foo2>\n",
            "[2023-02-28 17:18:54,611] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 17:18:54,612] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 17:18:54,612] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_109 <eval_with_key>.110 opcode         name    target                  args        kwargs\n",
            "-------------  ------  ----------------------  ----------  --------\n",
            "placeholder    a       a                       ()          {}\n",
            "call_method    sum_1   sum                     (a,)        {}\n",
            "call_function  lt      <built-in function lt>  (sum_1, 0)  {}\n",
            "output         output  output                  ((lt,),)    {}\n",
            "\n",
            "[2023-02-28 17:18:54,613] torch._dynamo.symbolic_convert: [INFO] __resume_at_42_110 function\n",
            "[2023-02-28 17:18:54,615] torch._dynamo.symbolic_convert: [INFO] __resume_at_50_111 function\n",
            "[2023-02-28 17:18:54,615] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2689548108.py line 3 \n",
            "  3           0 JUMP_ABSOLUTE           30\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               1 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                1 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               1 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6          42 LOAD_FAST                1 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               1 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                1 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               1 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                1 (x)\n",
            "             60 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:18:54,616] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2689548108.py line 3 \n",
            "  3           0 LOAD_GLOBAL              1 (__compiled_fn_109)\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 CALL_FUNCTION            1\n",
            "              6 UNPACK_SEQUENCE          1\n",
            "              8 POP_JUMP_IF_FALSE       18\n",
            "             10 LOAD_GLOBAL              2 (__resume_at_42_110)\n",
            "             12 LOAD_FAST                1 (x)\n",
            "             14 CALL_FUNCTION            1\n",
            "             16 RETURN_VALUE\n",
            "        >>   18 LOAD_GLOBAL              3 (__resume_at_50_111)\n",
            "             20 LOAD_FAST                1 (x)\n",
            "             22 CALL_FUNCTION            1\n",
            "             24 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:18:54,616] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'a' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338e0d1d0; to 'Tensor' at 0x7fa338d92e00>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            " - \n",
            "            local 'x' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338b7bcc0; to 'Tensor' at 0x7fa33b38b2c0>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            "[2023-02-28 17:18:54,618] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in foo2>\n",
            "[2023-02-28 17:18:54,619] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in foo2> (RETURN_VALUE)\n",
            "[2023-02-28 17:18:54,619] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 17:18:54,620] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 17:18:54,620] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_112 <eval_with_key>.112 opcode         name    target                   args       kwargs\n",
            "-------------  ------  -----------------------  ---------  --------\n",
            "placeholder    x       x                        ()         {}\n",
            "call_function  mul     <built-in function mul>  (2, x)     {}\n",
            "output         output  output                   ((mul,),)  {}\n",
            "\n",
            "[2023-02-28 17:18:54,621] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2689548108.py line 5 \n",
            "  5           0 JUMP_ABSOLUTE           50\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "             42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:18:54,621] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2689548108.py line 5 \n",
            "  5           0 LOAD_GLOBAL              1 (__compiled_fn_112)\n",
            "              2 LOAD_FAST                0 (x)\n",
            "              4 CALL_FUNCTION            1\n",
            "              6 UNPACK_SEQUENCE          1\n",
            "              8 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:18:54,621] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'x' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338b7bcc0; to 'Tensor' at 0x7fa33b38b2c0>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_compiler() called with FX graph-0:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/2689548108.py:2, code: x = a + b\n",
            "        add = a + b;  a = None\n",
            "        \n",
            "        # File: /tmp/ipykernel_3597807/2689548108.py:3, code: if b.sum() < 0:\n",
            "        sum_1 = b.sum();  b = None\n",
            "        lt = sum_1 < 0;  sum_1 = None\n",
            "        return (add, lt)\n",
            "        \n",
            "\n",
            "__resume_at_20_107:\n",
            "  3           0 JUMP_ABSOLUTE           22\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               1 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "\n",
            "  4     >>   22 LOAD_FAST                1 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               1 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6          42 LOAD_FAST                1 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               1 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                1 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               1 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                1 (x)\n",
            "             60 RETURN_VALUE\n",
            "__resume_at_28_108:\n",
            "  3           0 JUMP_ABSOLUTE           30\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               1 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                1 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               1 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6          42 LOAD_FAST                1 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               1 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                1 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               1 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                1 (x)\n",
            "             60 RETURN_VALUE\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338b8ebb0>\n",
            "my_compiler() called with FX graph-1:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, a : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/2689548108.py:5, code: if a.sum() < 0:\n",
            "        sum_1 = a.sum();  a = None\n",
            "        lt = sum_1 < 0;  sum_1 = None\n",
            "        return (lt,)\n",
            "        \n",
            "\n",
            "__resume_at_42_110:\n",
            "  5           0 JUMP_ABSOLUTE           42\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6     >>   42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "__resume_at_50_111:\n",
            "  5           0 JUMP_ABSOLUTE           50\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "             42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338b33040>\n",
            "my_compiler() called with FX graph-2:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, x : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/2689548108.py:7, code: x = 2 * x\n",
            "        mul = 2 * x;  x = None\n",
            "        return (mul,)\n",
            "        \n",
            "\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338b39a90>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[4., 4., 4.],\n",
              "        [4., 4., 4.]])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch._dynamo.reset() # reset all che compilation cache\n",
        "my_graph_id = 0\n",
        "\n",
        "a = torch.ones((2, 3))\n",
        "b = torch.ones((2, 3))\n",
        "\n",
        "# It should tigger only one case of the if-else\n",
        "foo2_(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gS_r-Wf-XlzO",
      "metadata": {
        "id": "gS_r-Wf-XlzO"
      },
      "source": [
        "### Exectue all the cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "bnsXks_iUy3K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnsXks_iUy3K",
        "outputId": "c7ca2e17-4a49-49d4-f98c-0fbc1ec6795e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-02-28 17:18:59,100] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in foo2>\n",
            "[2023-02-28 17:18:59,102] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in foo2> (RETURN_VALUE)\n",
            "[2023-02-28 17:18:59,104] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 17:18:59,105] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 17:18:59,105] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_113 <eval_with_key>.114 opcode         name    target                   args         kwargs\n",
            "-------------  ------  -----------------------  -----------  --------\n",
            "placeholder    x       x                        ()           {}\n",
            "call_function  mul     <built-in function mul>  (x, -2)      {}\n",
            "call_function  mul_1   <built-in function mul>  (2, mul)     {}\n",
            "output         output  output                   ((mul_1,),)  {}\n",
            "\n",
            "[2023-02-28 17:18:59,106] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2689548108.py line 5 \n",
            "  5           0 JUMP_ABSOLUTE           42\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6     >>   42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:18:59,106] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2689548108.py line 5 \n",
            "  5           0 LOAD_GLOBAL              1 (__compiled_fn_113)\n",
            "              2 LOAD_FAST                0 (x)\n",
            "              4 CALL_FUNCTION            1\n",
            "              6 UNPACK_SEQUENCE          1\n",
            "              8 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:18:59,107] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'x' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338b31950; to 'Tensor' at 0x7fa338d40180>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_compiler() called with FX graph-3:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, x : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/2689548108.py:6, code: x = x * -2\n",
            "        mul = x * -2;  x = None\n",
            "        \n",
            "        # File: /tmp/ipykernel_3597807/2689548108.py:7, code: x = 2 * x\n",
            "        mul_1 = 2 * mul;  mul = None\n",
            "        return (mul_1,)\n",
            "        \n",
            "\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338b19340>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0., -0., -0.],\n",
              "        [-0., -0., -0.]])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#torch._dynamo.reset() # reset all che compilation cache\n",
        "#my_graph_id = 0\n",
        "\n",
        "# It should tigger all the four combinations of the if-conditions\n",
        "#foo2_(a, b)\n",
        "foo2_(-a, b)\n",
        "#foo2_(-a, b)\n",
        "#foo2_(-a, -b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "5c5d9c5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-02-28 17:19:37,187] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in foo2>\n",
            "[2023-02-28 17:19:37,191] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 17:19:37,192] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 17:19:37,193] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_114 <eval_with_key>.116 opcode         name    target                   args          kwargs\n",
            "-------------  ------  -----------------------  ------------  --------\n",
            "placeholder    a       a                        ()            {}\n",
            "placeholder    x       x                        ()            {}\n",
            "call_function  mul     <built-in function mul>  (x, -1)       {}\n",
            "call_method    sum_1   sum                      (a,)          {}\n",
            "call_function  lt      <built-in function lt>   (sum_1, 0)    {}\n",
            "output         output  output                   ((mul, lt),)  {}\n",
            "\n",
            "[2023-02-28 17:19:37,193] torch._dynamo.symbolic_convert: [INFO] __resume_at_42_115 function\n",
            "[2023-02-28 17:19:37,194] torch._dynamo.symbolic_convert: [INFO] __resume_at_50_116 function\n",
            "[2023-02-28 17:19:37,195] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2689548108.py line 3 \n",
            "  3           0 JUMP_ABSOLUTE           22\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               1 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "\n",
            "  4     >>   22 LOAD_FAST                1 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               1 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6          42 LOAD_FAST                1 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               1 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                1 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               1 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                1 (x)\n",
            "             60 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:19:37,195] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2689548108.py line 3 \n",
            "  3           0 LOAD_GLOBAL              1 (__compiled_fn_114)\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                1 (x)\n",
            "              6 CALL_FUNCTION            2\n",
            "              8 UNPACK_SEQUENCE          2\n",
            "             10 STORE_FAST               1 (x)\n",
            "             12 POP_JUMP_IF_FALSE       22\n",
            "             14 LOAD_GLOBAL              2 (__resume_at_42_115)\n",
            "             16 LOAD_FAST                1 (x)\n",
            "             18 CALL_FUNCTION            1\n",
            "             20 RETURN_VALUE\n",
            "        >>   22 LOAD_GLOBAL              3 (__resume_at_50_116)\n",
            "             24 LOAD_FAST                1 (x)\n",
            "             26 CALL_FUNCTION            1\n",
            "             28 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 17:19:37,196] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'a' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338e0d1d0; to 'Tensor' at 0x7fa338d92e00>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            " - \n",
            "            local 'x' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338b31b30; to 'Tensor' at 0x7fa338b11900>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_compiler() called with FX graph-4:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, a : torch.Tensor, x : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/2689548108.py:4, code: x = x * -1\n",
            "        mul = x * -1;  x = None\n",
            "        \n",
            "        # File: /tmp/ipykernel_3597807/2689548108.py:5, code: if a.sum() < 0:\n",
            "        sum_1 = a.sum();  a = None\n",
            "        lt = sum_1 < 0;  sum_1 = None\n",
            "        return (mul, lt)\n",
            "        \n",
            "\n",
            "__resume_at_42_115:\n",
            "  5           0 JUMP_ABSOLUTE           42\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6     >>   42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "__resume_at_50_116:\n",
            "  5           0 JUMP_ABSOLUTE           50\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "             42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               3 (-2)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               4 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338b67700>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0., -0., -0.],\n",
              "        [-0., -0., -0.]])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "foo2_(a, -b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ec4d0c81",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-8., -8., -8.],\n",
              "        [-8., -8., -8.]])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "foo2_(-a, -b)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bc849206",
      "metadata": {},
      "source": [
        "### Python Binary Code for this example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "50eb1bf9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  2           0 LOAD_FAST                0 (a)\n",
            "              2 LOAD_FAST                1 (b)\n",
            "              4 BINARY_ADD\n",
            "              6 STORE_FAST               2 (x)\n",
            "\n",
            "  3           8 LOAD_FAST                1 (b)\n",
            "             10 LOAD_METHOD              0 (sum)\n",
            "             12 CALL_METHOD              0\n",
            "             14 LOAD_CONST               1 (0)\n",
            "             16 COMPARE_OP               0 (<)\n",
            "             18 POP_JUMP_IF_FALSE       28\n",
            "\n",
            "  4          20 LOAD_FAST                2 (x)\n",
            "             22 LOAD_CONST               2 (-1)\n",
            "             24 BINARY_MULTIPLY\n",
            "             26 STORE_FAST               2 (x)\n",
            "\n",
            "  5     >>   28 LOAD_FAST                0 (a)\n",
            "             30 LOAD_METHOD              0 (sum)\n",
            "             32 CALL_METHOD              0\n",
            "             34 LOAD_CONST               1 (0)\n",
            "             36 COMPARE_OP               0 (<)\n",
            "             38 POP_JUMP_IF_FALSE       48\n",
            "\n",
            "  6          40 LOAD_FAST                2 (x)\n",
            "             42 LOAD_CONST               2 (-1)\n",
            "             44 BINARY_MULTIPLY\n",
            "             46 STORE_FAST               2 (x)\n",
            "\n",
            "  7     >>   48 LOAD_CONST               3 (2)\n",
            "             50 LOAD_FAST                2 (x)\n",
            "             52 BINARY_MULTIPLY\n",
            "             54 STORE_FAST               2 (x)\n",
            "\n",
            "  8          56 LOAD_FAST                2 (x)\n",
            "             58 RETURN_VALUE\n"
          ]
        }
      ],
      "source": [
        "import dis\n",
        "dis.dis(foo2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "t8oFkOuYVEmk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8oFkOuYVEmk",
        "outputId": "daf6add5-3292-4fb2-f1f4-230517d4f211"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2023-02-28 16:44:15,936] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing foo2\n",
            "[2023-02-28 16:44:15,942] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 16:44:15,942] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 16:44:15,943] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_77 <eval_with_key>.82 opcode         name    target                   args          kwargs\n",
            "-------------  ------  -----------------------  ------------  --------\n",
            "placeholder    a       a                        ()            {}\n",
            "placeholder    b       b                        ()            {}\n",
            "call_function  add     <built-in function add>  (a, b)        {}\n",
            "call_method    sum_1   sum                      (b,)          {}\n",
            "call_function  lt      <built-in function lt>   (sum_1, 0)    {}\n",
            "output         output  output                   ((add, lt),)  {}\n",
            "\n",
            "[2023-02-28 16:44:15,944] torch._dynamo.symbolic_convert: [INFO] __resume_at_20_78 function\n",
            "[2023-02-28 16:44:15,945] torch._dynamo.symbolic_convert: [INFO] __resume_at_28_79 function\n",
            "[2023-02-28 16:44:15,946] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE foo2 /tmp/ipykernel_3597807/2473059898.py line 1 \n",
            "  2           0 LOAD_FAST                0 (a)\n",
            "              2 LOAD_FAST                1 (b)\n",
            "              4 BINARY_ADD\n",
            "              6 STORE_FAST               2 (x)\n",
            "\n",
            "  3           8 LOAD_FAST                1 (b)\n",
            "             10 LOAD_METHOD              0 (sum)\n",
            "             12 CALL_METHOD              0\n",
            "             14 LOAD_CONST               1 (0)\n",
            "             16 COMPARE_OP               0 (<)\n",
            "             18 POP_JUMP_IF_FALSE       28\n",
            "\n",
            "  4          20 LOAD_FAST                2 (x)\n",
            "             22 LOAD_CONST               2 (-1)\n",
            "             24 BINARY_MULTIPLY\n",
            "             26 STORE_FAST               2 (x)\n",
            "\n",
            "  5     >>   28 LOAD_FAST                0 (a)\n",
            "             30 LOAD_METHOD              0 (sum)\n",
            "             32 CALL_METHOD              0\n",
            "             34 LOAD_CONST               1 (0)\n",
            "             36 COMPARE_OP               0 (<)\n",
            "             38 POP_JUMP_IF_FALSE       48\n",
            "\n",
            "  6          40 LOAD_FAST                2 (x)\n",
            "             42 LOAD_CONST               2 (-1)\n",
            "             44 BINARY_MULTIPLY\n",
            "             46 STORE_FAST               2 (x)\n",
            "\n",
            "  7     >>   48 LOAD_CONST               3 (2)\n",
            "             50 LOAD_FAST                2 (x)\n",
            "             52 BINARY_MULTIPLY\n",
            "             54 STORE_FAST               2 (x)\n",
            "\n",
            "  8          56 LOAD_FAST                2 (x)\n",
            "             58 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:44:15,946] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE foo2 /tmp/ipykernel_3597807/2473059898.py line 1 \n",
            "  1           0 LOAD_GLOBAL              1 (__compiled_fn_77)\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                1 (b)\n",
            "              6 CALL_FUNCTION            2\n",
            "              8 UNPACK_SEQUENCE          2\n",
            "             10 STORE_FAST               2 (x)\n",
            "             12 POP_JUMP_IF_FALSE       24\n",
            "             14 LOAD_GLOBAL              2 (__resume_at_20_78)\n",
            "             16 LOAD_FAST                0 (a)\n",
            "             18 LOAD_FAST                2 (x)\n",
            "             20 CALL_FUNCTION            2\n",
            "             22 RETURN_VALUE\n",
            "        >>   24 LOAD_GLOBAL              3 (__resume_at_28_79)\n",
            "             26 LOAD_FAST                0 (a)\n",
            "             28 LOAD_FAST                2 (x)\n",
            "             30 CALL_FUNCTION            2\n",
            "             32 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:44:15,947] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'a' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa2c6722040; to 'Tensor' at 0x7fa338d40680>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            " - \n",
            "            local 'b' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338e0d7c0; to 'Tensor' at 0x7fa338beaa40>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            "[2023-02-28 16:44:15,949] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in foo2>\n",
            "[2023-02-28 16:44:15,951] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 16:44:15,952] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 16:44:15,952] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_80 <eval_with_key>.84 opcode         name    target                  args        kwargs\n",
            "-------------  ------  ----------------------  ----------  --------\n",
            "placeholder    a       a                       ()          {}\n",
            "call_method    sum_1   sum                     (a,)        {}\n",
            "call_function  lt      <built-in function lt>  (sum_1, 0)  {}\n",
            "output         output  output                  ((lt,),)    {}\n",
            "\n",
            "[2023-02-28 16:44:15,953] torch._dynamo.symbolic_convert: [INFO] __resume_at_42_81 function\n",
            "[2023-02-28 16:44:15,955] torch._dynamo.symbolic_convert: [INFO] __resume_at_50_82 function\n",
            "[2023-02-28 16:44:15,955] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2473059898.py line 3 \n",
            "  3           0 JUMP_ABSOLUTE           30\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               1 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                1 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               1 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6          42 LOAD_FAST                1 (x)\n",
            "             44 LOAD_CONST               2 (-1)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               1 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               3 (2)\n",
            "             52 LOAD_FAST                1 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               1 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                1 (x)\n",
            "             60 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:44:15,956] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2473059898.py line 3 \n",
            "  3           0 LOAD_GLOBAL              1 (__compiled_fn_80)\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 CALL_FUNCTION            1\n",
            "              6 UNPACK_SEQUENCE          1\n",
            "              8 POP_JUMP_IF_FALSE       18\n",
            "             10 LOAD_GLOBAL              2 (__resume_at_42_81)\n",
            "             12 LOAD_FAST                1 (x)\n",
            "             14 CALL_FUNCTION            1\n",
            "             16 RETURN_VALUE\n",
            "        >>   18 LOAD_GLOBAL              3 (__resume_at_50_82)\n",
            "             20 LOAD_FAST                1 (x)\n",
            "             22 CALL_FUNCTION            1\n",
            "             24 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:44:15,956] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'a' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa2c6722040; to 'Tensor' at 0x7fa338d40680>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            " - \n",
            "            local 'x' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338c01720; to 'Tensor' at 0x7fa338d510e0>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n",
            "[2023-02-28 16:44:15,958] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing <graph break in foo2>\n",
            "[2023-02-28 16:44:15,959] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing <graph break in foo2> (RETURN_VALUE)\n",
            "[2023-02-28 16:44:15,960] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function my_compiler\n",
            "[2023-02-28 16:44:15,961] torch._dynamo.output_graph: [INFO] Step 2: done compiler function my_compiler\n",
            "[2023-02-28 16:44:15,961] torch._dynamo.output_graph: [INFO] TRACED GRAPH\n",
            " __compiled_fn_83 <eval_with_key>.86 opcode         name    target                   args         kwargs\n",
            "-------------  ------  -----------------------  -----------  --------\n",
            "placeholder    x       x                        ()           {}\n",
            "call_function  mul     <built-in function mul>  (x, -1)      {}\n",
            "call_function  mul_1   <built-in function mul>  (2, mul)     {}\n",
            "output         output  output                   ((mul_1,),)  {}\n",
            "\n",
            "[2023-02-28 16:44:15,962] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2473059898.py line 5 \n",
            "  5           0 JUMP_ABSOLUTE           42\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6     >>   42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               2 (-1)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               3 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:44:15,962] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE <graph break in foo2> /tmp/ipykernel_3597807/2473059898.py line 5 \n",
            "  5           0 LOAD_GLOBAL              1 (__compiled_fn_83)\n",
            "              2 LOAD_FAST                0 (x)\n",
            "              4 CALL_FUNCTION            1\n",
            "              6 UNPACK_SEQUENCE          1\n",
            "              8 RETURN_VALUE\n",
            "\n",
            " \n",
            "[2023-02-28 16:44:15,963] torch._dynamo.convert_frame: [INFO] GUARDS:\n",
            " - \n",
            "            local 'x' TENSOR_MATCH\n",
            "            {\n",
            "                'guard_types': ['TENSOR_MATCH'],\n",
            "                'code': None,\n",
            "                'obj_weakref': <weakref at 0x7fa338c01720; to 'Tensor' at 0x7fa338d510e0>\n",
            "                'guarded_class': <weakref at 0x7fa403aa9a90; to 'torch._C._TensorMeta' at 0x652f020 (Tensor)>\n",
            "            }\n",
            "            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "my_compiler() called with FX graph-0:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, a : torch.Tensor, b : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/2473059898.py:2, code: x = a + b\n",
            "        add = a + b;  a = None\n",
            "        \n",
            "        # File: /tmp/ipykernel_3597807/2473059898.py:3, code: if b.sum() < 0:\n",
            "        sum_1 = b.sum();  b = None\n",
            "        lt = sum_1 < 0;  sum_1 = None\n",
            "        return (add, lt)\n",
            "        \n",
            "\n",
            "__resume_at_20_78:\n",
            "  3           0 JUMP_ABSOLUTE           22\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               1 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "\n",
            "  4     >>   22 LOAD_FAST                1 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               1 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6          42 LOAD_FAST                1 (x)\n",
            "             44 LOAD_CONST               2 (-1)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               1 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               3 (2)\n",
            "             52 LOAD_FAST                1 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               1 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                1 (x)\n",
            "             60 RETURN_VALUE\n",
            "__resume_at_28_79:\n",
            "  3           0 JUMP_ABSOLUTE           30\n",
            "              2 LOAD_FAST                0 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               1 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                1 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               1 (x)\n",
            "\n",
            "  5     >>   30 LOAD_FAST                0 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6          42 LOAD_FAST                1 (x)\n",
            "             44 LOAD_CONST               2 (-1)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               1 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               3 (2)\n",
            "             52 LOAD_FAST                1 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               1 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                1 (x)\n",
            "             60 RETURN_VALUE\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338be74f0>\n",
            "my_compiler() called with FX graph-1:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, a : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/2473059898.py:5, code: if a.sum() < 0:\n",
            "        sum_1 = a.sum();  a = None\n",
            "        lt = sum_1 < 0;  sum_1 = None\n",
            "        return (lt,)\n",
            "        \n",
            "\n",
            "__resume_at_42_81:\n",
            "  5           0 JUMP_ABSOLUTE           42\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "\n",
            "  6     >>   42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               2 (-1)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               3 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "__resume_at_50_82:\n",
            "  5           0 JUMP_ABSOLUTE           50\n",
            "              2 LOAD_FAST                1 (a)\n",
            "              4 LOAD_FAST                2 (b)\n",
            "              6 BINARY_ADD\n",
            "              8 STORE_FAST               0 (x)\n",
            "             10 LOAD_FAST                2 (b)\n",
            "             12 LOAD_ATTR                0 (sum)\n",
            "             14 CALL_FUNCTION            0\n",
            "             16 LOAD_CONST               1 (0)\n",
            "             18 COMPARE_OP               0 (<)\n",
            "             20 POP_JUMP_IF_FALSE       30\n",
            "             22 LOAD_FAST                0 (x)\n",
            "             24 LOAD_CONST               2 (-1)\n",
            "             26 BINARY_MULTIPLY\n",
            "             28 STORE_FAST               0 (x)\n",
            "        >>   30 LOAD_FAST                1 (a)\n",
            "             32 LOAD_ATTR                0 (sum)\n",
            "             34 CALL_FUNCTION            0\n",
            "             36 LOAD_CONST               1 (0)\n",
            "             38 COMPARE_OP               0 (<)\n",
            "             40 POP_JUMP_IF_FALSE       50\n",
            "             42 LOAD_FAST                0 (x)\n",
            "             44 LOAD_CONST               2 (-1)\n",
            "             46 BINARY_MULTIPLY\n",
            "             48 STORE_FAST               0 (x)\n",
            "\n",
            "  7     >>   50 LOAD_CONST               3 (2)\n",
            "             52 LOAD_FAST                0 (x)\n",
            "             54 BINARY_MULTIPLY\n",
            "             56 STORE_FAST               0 (x)\n",
            "\n",
            "  8          58 LOAD_FAST                0 (x)\n",
            "             60 RETURN_VALUE\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338d45d30>\n",
            "my_compiler() called with FX graph-2:\n",
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, x : torch.Tensor):\n",
            "        # File: /tmp/ipykernel_3597807/2473059898.py:6, code: x = x * -1\n",
            "        mul = x * -1;  x = None\n",
            "        \n",
            "        # File: /tmp/ipykernel_3597807/2473059898.py:7, code: x = 2 * x\n",
            "        mul_1 = 2 * mul;  mul = None\n",
            "        return (mul_1,)\n",
            "        \n",
            "\n",
            "transform.output <torch._dynamo.output_graph.OutputGraph object at 0x7fa338b800d0>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.4001, -0.2423, -4.1457],\n",
              "        [ 1.0497, -1.7117,  1.4444]])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch._dynamo.reset() # reset all che compilation cache\n",
        "my_graph_id = 0\n",
        "\n",
        "a = torch.randn((2, 3))\n",
        "b = torch.randn((2, 3))\n",
        "\n",
        "# It should tigger only one case of the if-else\n",
        "foo2_(a, b)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e8e5476a",
      "metadata": {
        "id": "e8e5476a"
      },
      "source": [
        "## bytecode Instructions to FX Graph"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "016b3e06",
      "metadata": {},
      "source": [
        "The execution pass:\n",
        "\n",
        "1. optimize()\n",
        "2. convert_frame(), Try to convert a frame into an FX graph, if error leave frame unmodified.\n",
        "   1. `result = inner_convert(frame, cache_size, hooks)`\n",
        "3. convert_frame_assert()\n",
        "4. _compile(), it gets an CompileFn which takes an `fx.GraphModule` as input and outputs a list of `torch.Tensor`.\n",
        "   1. transform()\n",
        "      1. `tracer = InstructionTranslator(...)`\n",
        "      2. `tracer.run()`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b8b78e80",
      "metadata": {},
      "source": [
        "Get the instructions from foo2 Code Object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dadf8273",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Instruction(opcode=124, opname='LOAD_FAST', arg=0, argval='a', offset=0, starts_line=2, is_jump_target=False, target=None),\n",
              " Instruction(opcode=124, opname='LOAD_FAST', arg=1, argval='b', offset=2, starts_line=2, is_jump_target=False, target=None),\n",
              " Instruction(opcode=23, opname='BINARY_ADD', arg=None, argval=None, offset=4, starts_line=2, is_jump_target=False, target=None),\n",
              " Instruction(opcode=125, opname='STORE_FAST', arg=2, argval='x', offset=6, starts_line=2, is_jump_target=False, target=None),\n",
              " Instruction(opcode=124, opname='LOAD_FAST', arg=1, argval='b', offset=8, starts_line=3, is_jump_target=False, target=None),\n",
              " Instruction(opcode=106, opname='LOAD_ATTR', arg=0, argval='sum', offset=10, starts_line=3, is_jump_target=False, target=None),\n",
              " Instruction(opcode=131, opname='CALL_FUNCTION', arg=0, argval=0, offset=12, starts_line=3, is_jump_target=False, target=None),\n",
              " Instruction(opcode=100, opname='LOAD_CONST', arg=1, argval=0, offset=14, starts_line=3, is_jump_target=False, target=None),\n",
              " Instruction(opcode=107, opname='COMPARE_OP', arg=0, argval='<', offset=16, starts_line=3, is_jump_target=False, target=None),\n",
              " Instruction(opcode=114, opname='POP_JUMP_IF_FALSE', arg=28, argval=28, offset=18, starts_line=3, is_jump_target=False, target=Instruction(opcode=124, opname='LOAD_FAST', arg=0, argval='a', offset=28, starts_line=5, is_jump_target=True, target=None)),\n",
              " Instruction(opcode=124, opname='LOAD_FAST', arg=2, argval='x', offset=20, starts_line=4, is_jump_target=False, target=None),\n",
              " Instruction(opcode=100, opname='LOAD_CONST', arg=2, argval=-1, offset=22, starts_line=4, is_jump_target=False, target=None),\n",
              " Instruction(opcode=20, opname='BINARY_MULTIPLY', arg=None, argval=None, offset=24, starts_line=4, is_jump_target=False, target=None),\n",
              " Instruction(opcode=125, opname='STORE_FAST', arg=2, argval='x', offset=26, starts_line=4, is_jump_target=False, target=None),\n",
              " Instruction(opcode=124, opname='LOAD_FAST', arg=0, argval='a', offset=28, starts_line=5, is_jump_target=True, target=None),\n",
              " Instruction(opcode=106, opname='LOAD_ATTR', arg=0, argval='sum', offset=30, starts_line=5, is_jump_target=False, target=None),\n",
              " Instruction(opcode=131, opname='CALL_FUNCTION', arg=0, argval=0, offset=32, starts_line=5, is_jump_target=False, target=None),\n",
              " Instruction(opcode=100, opname='LOAD_CONST', arg=1, argval=0, offset=34, starts_line=5, is_jump_target=False, target=None),\n",
              " Instruction(opcode=107, opname='COMPARE_OP', arg=0, argval='<', offset=36, starts_line=5, is_jump_target=False, target=None),\n",
              " Instruction(opcode=114, opname='POP_JUMP_IF_FALSE', arg=48, argval=48, offset=38, starts_line=5, is_jump_target=False, target=Instruction(opcode=100, opname='LOAD_CONST', arg=3, argval=2, offset=48, starts_line=7, is_jump_target=True, target=None)),\n",
              " Instruction(opcode=124, opname='LOAD_FAST', arg=2, argval='x', offset=40, starts_line=6, is_jump_target=False, target=None),\n",
              " Instruction(opcode=100, opname='LOAD_CONST', arg=2, argval=-1, offset=42, starts_line=6, is_jump_target=False, target=None),\n",
              " Instruction(opcode=20, opname='BINARY_MULTIPLY', arg=None, argval=None, offset=44, starts_line=6, is_jump_target=False, target=None),\n",
              " Instruction(opcode=125, opname='STORE_FAST', arg=2, argval='x', offset=46, starts_line=6, is_jump_target=False, target=None),\n",
              " Instruction(opcode=100, opname='LOAD_CONST', arg=3, argval=2, offset=48, starts_line=7, is_jump_target=True, target=None),\n",
              " Instruction(opcode=124, opname='LOAD_FAST', arg=2, argval='x', offset=50, starts_line=7, is_jump_target=False, target=None),\n",
              " Instruction(opcode=20, opname='BINARY_MULTIPLY', arg=None, argval=None, offset=52, starts_line=7, is_jump_target=False, target=None),\n",
              " Instruction(opcode=125, opname='STORE_FAST', arg=2, argval='x', offset=54, starts_line=7, is_jump_target=False, target=None),\n",
              " Instruction(opcode=124, opname='LOAD_FAST', arg=2, argval='x', offset=56, starts_line=8, is_jump_target=False, target=None),\n",
              " Instruction(opcode=83, opname='RETURN_VALUE', arg=None, argval=None, offset=58, starts_line=8, is_jump_target=False, target=None)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch._dynamo.bytecode_transformation import cleaned_instructions\n",
        "from torch._dynamo.bytecode_analysis import propagate_line_nums\n",
        "\n",
        "instructions = cleaned_instructions(foo2.__code__)\n",
        "propagate_line_nums(instructions)\n",
        "instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6ac51d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "trienv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "9be45825b6e11033c42eb29377956e200d55264a3cce733a812afa9001a7e64f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
